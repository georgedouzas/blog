<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI and friends</title>
    <link>https://georgedouzas.github.io/blog/</link>
      <atom:link href="https://georgedouzas.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <description>AI and friends</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 17 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>AI and friends</title>
      <link>https://georgedouzas.github.io/blog/</link>
    </image>
    
    <item>
      <title>Imbalanced Learning in Land Cover Classification: Improving Minority Classes’ Prediction Accuracy Using the Geometric SMOTE Algorithm</title>
      <link>https://georgedouzas.github.io/blog/publication/land_cover_classification/</link>
      <pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://georgedouzas.github.io/blog/publication/land_cover_classification/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Geometric SMOTE a geometrically enhanced drop-in replacement for SMOTE</title>
      <link>https://georgedouzas.github.io/blog/publication/gsmote_journal/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://georgedouzas.github.io/blog/publication/gsmote_journal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Geometric SMOTE</title>
      <link>https://georgedouzas.github.io/blog/project/geometric-smote/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://georgedouzas.github.io/blog/project/geometric-smote/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Classification of imbalanced datasets is a challenging task for standard machine learning algorithms. Training a classifier on imbalanced data, often results in a low out-of-sample accuracy for the minority classes. To deal with this problem several approaches have been proposed. A general approach, known as oversampling, is the generation of artificial data for the minority classes that are used to enhance the training data.&lt;/p&gt;
&lt;p&gt;SMOTE is the most popular oversampler, while many variants of it have been proposed. Geometric SMOTE, a geometric modification of the SMOTE data generation mechanism, is a state-of-the-art oversampling algorithm that 
&lt;a href=&#34;../../publication/gsmote_journal&#34;&gt;has been shown&lt;/a&gt; to outperform other standard oversamplers in a large number of datasets.&lt;/p&gt;
&lt;p&gt;A Python implementation of SMOTE and several of its variants is available in the 
&lt;a href=&#34;https://imbalanced-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbalanced-Learn&lt;/a&gt; library, which is fully compatible with the popular machine learning toolbox 
&lt;a href=&#34;https://scikit-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scikit-Learn&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I have developed a Python implementation of Geometric SMOTE oversampler, called &lt;code&gt;geometric-smote&lt;/code&gt;, that integrates seamlessly with the Scikit-Learn and Imblanced-Learn ecosystems.&lt;/p&gt;
&lt;h2 id=&#34;usage&#34;&gt;Usage&lt;/h2&gt;
&lt;p&gt;Detailed documentation that includes installation guidelines, API description and various examples can found 
&lt;a href=&#34;https://geometric-smote.readthedocs.io/en/latest/?badge=latest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. In what follows, I will describe briefly some aspects of &lt;code&gt;geometric-smote&lt;/code&gt;&#39;s functionality.&lt;/p&gt;
&lt;p&gt;The class that represents the Geometric SMOTE oversampler is called &lt;code&gt;GeometricSMOTE&lt;/code&gt;. Its API follows closely the API of oversamplers provided by Imbalanced-Learn. In order to show its functionality I will initially generate some binary class imbalanced data, represented by the input matrix &lt;code&gt;X&lt;/code&gt; and the target vector &lt;code&gt;y&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import scikit-learn
from sklearn.datasets import make_classification

# Set random seed
rnd_seed = 40

# Generate imbalanced data
X, y = make_classification(
  n_classes=2,
  weights=[0.9, 0.1],
  random_state=rnd_seed
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following snippet prints the main characteristics of the dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import Counter
from collections import Counter

# Print dataset&#39;s characteristics
print(
  &#39;Number of samples&#39;
)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Το φαινόμενο Dunning-Krugger και ο Ηλίας Μάσκας</title>
      <link>https://georgedouzas.github.io/blog/post/dunning-krugger/</link>
      <pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://georgedouzas.github.io/blog/post/dunning-krugger/</guid>
      <description>&lt;h2 id=&#34;αυστηρός-ορισμός-του-προβλήματος&#34;&gt;Αυστηρός ορισμός του προβλήματος&lt;/h2&gt;
&lt;p&gt;Όλοι έχουμε βρεθεί σε μια παρέα στην οποία κάποιος φαίνεται να έχει απάντησεις για όλα τα θέματα. Έστω ότι τον λένε Μάκη. Ο Μάκης λοιπόν γνωρίζει απο διαστημική τεχνολογία, άρμεγμα της γίδας, διαφορική γεωμετρία σε πολλαπλότητες Calabi-Yau και οποιοδήποτε άλλο θέμα προκύψει πάνω στην κουβέντα. Όχι μόνο έχει απόψη αλλά μιλάει με την σιγουριά που θα εξηγούσε ένας νομπελίστας φυσικός κάποια θεωρία την οποία διατύπωσε ο ίδιος. Με ορατό τον κινδυνο να σε προσβάλλω και να σε χάσω απο πελάτη, θέλω να τονίσω ότι αν δεν αναγνωρίζεις κάποιον Μάκη στην παρέα σου είναι γιατί προφανώς ο Μάκης είσαι εσύ.&lt;/p&gt;
&lt;h2 id=&#34;η-επιστημονική-εξήγηση&#34;&gt;Η επιστημονική εξήγηση&lt;/h2&gt;
&lt;p&gt;Που θέλω να καταλήξω με όλα αυτά; Πολύ απλά και λαικά, το φαινόμενο του άχρηστου μπετόβλακα ο οποίος θεωρεί ότι ξέρει τα πάντα έχει ονομασία: Φαινόμενο Ντάνινγκ–Κρούγκερ. Απο την πάντα αξιόπιστη wikipedia:&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Το φαινόμενο Ντάνινγκ–Κρούγκερ είναι μια γνωστική διαταραχή στην οποία άτομα περιορισμένων δεξιοτήτων αποκτούν μια ψευδαίσθηση ανωτερώτητας, εκτιμώντας εσφαλμένα οτι οι γνωστικές τους ικανότητες, είναι υψηλότερες από ό,τι πραγματικά είναι.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Επειδή σε ότι κάνω χρησιμοποιώ αποκλειστικά την επιστημονική μέθοδο, το έψαξα αρκετά και τελικά κατάλαβα ότι ο ακαδημαικός Ντάνινγκ–Κρούγκερ είναι δύο διαφορετικοί άνθρωποι. Χωρίς να θέλω να κάνω τον έξυπνο, είναι πολυ λογικό γιατι υπάρχει η &amp;ldquo;-&amp;rdquo; ανάμεσα στα ονόματα. Έχεις δει ποτέ να γράφουμε Ζαν-Κλοντ-Βαν-Νταμ; Τέλος πάντων οι Ντάνινγκ–Κρούγκερ δεν μελέτησαν μόνο τους ηλίθιους αλλά και τους έξυπνους:&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Άτομα υψηλής ειδίκευσης μπορεί να υποτιμούν τη δική τους σχετική ικανότητά και μπορεί εσφαλμένα να υποθέτουν ότι τα καθήκοντα τα οποία είναι εύκολα για αυτούς είναι επίσης εύκολα για τους άλλους.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Το ζουμί της ιστορίας είναι το παρακάτω γράφημα:&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://georgedouzas.github.io/blog/blog/post/dunning-krugger/simple-dn_hu0e5feadbcfd7edd8ed8410ba9b0c9b0a_85348_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Ωραίο γράφημα αλλά είναι παραμύθι.&#34;&gt;


  &lt;img data-src=&#34;https://georgedouzas.github.io/blog/blog/post/dunning-krugger/simple-dn_hu0e5feadbcfd7edd8ed8410ba9b0c9b0a_85348_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;700px&#34; height=&#34;425&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Ωραίο γράφημα αλλά είναι παραμύθι.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Άρα βλέπουμε ότι αρχικά ο άχρηστος δεν ξέρει ούτε το όνομά του και προφανώς έχει μηδέν αυτοπεποίθηση. Με το που θα μάθει δύo πράγματα, η αυτοπεποίθησή του γρήγορα μέγιστοποιείται. Όταν λέω δύο πράγματα εννοώ να ανοίγει μια κονσέρβα ή να αλλάζει κανάλι. Αισθάνεται αυθεντία. Αν τελικά αποφασίσει να μάθει κάτι παραπάνω τότε καταλαβαίνει ότι δεν έχει ιδέα και δεν νιώθει σιγουριά για τις ικανότητές του. Στα δεξιά της καμπύλης ανήκουν οι πραγματικές αυθεντίες, όπως είναι ο Άδωνις Γεωργιάδης ή ο Νίκος Καρβέλας. Έχουν μπόλικη αυτοπεποίθηση αλλά σαφώς μικρότερη από αυτή που διαθέτει ο γκασμάς.&lt;/p&gt;
&lt;p&gt;Υποτίθεται ότι το γράφημα συνοψίζει με απλό τρόπο το φαινόμενο Ντάνινγκ–Κρούγκερ και αν το κοιτάξεις θα δείς ότι αναφέρεται σχεδόν σε όλα τα σχετικα άρθρα. Δυστυχώς ένα προβληματάκι
που έχει είναι ότι δεν βρίσκεται στις δημοσίευσεις των Ντάνινγκ–Κρούγκερ. Κάποιος το ζωγράφισε
στο Paint και οι υπόλοιποι το πίστεψαν. Ελπίζω δηλαδή, γιατί εγώ σίγουρα το πίστεψα. Εντάξει δεν πειράζει, ο σκοπός δεν είναι να δείξουμε τι ισχύει αλλά τι μας συμφέρει να ισχύει.
Να ένα πραγματικό γράφημα:&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://georgedouzas.github.io/blog/blog/post/dunning-krugger/real-dn_huec9932930f769cd520f97ebec17b6af8_21871_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Αυτό είναι το πραγματικό ναούμε.&#34;&gt;


  &lt;img data-src=&#34;https://georgedouzas.github.io/blog/blog/post/dunning-krugger/real-dn_huec9932930f769cd520f97ebec17b6af8_21871_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;700px&#34; height=&#34;246&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Αυτό είναι το πραγματικό ναούμε.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Το τελευταίο δείχνει ότι όσοι λένε κρυάδες θεωρούν ότι έχουν αρκετή πλάκα ενώ οι Σεφερλήδες υποτιμούν το πόσο αστείοι είναι. ΧΑΧΑΧΑΑΧΧΑΑΧ. Ξέσπασα.
Αντίστοιχα γραφήματα υπάρχουν και για άλλες δεξιότητες. Δηλαδή ο άχρηστος δεν θεωρεί ότι είναι αυθεντία αλλά ότι είναι καλύτερος απο το μέσο όρο.
Αυτό ακούγεται πιο λογικό σε σχέση με το προηγούμενο.&lt;/p&gt;
&lt;p&gt;Μεταγενέστερες μελέτες ασκούν κριτική στα ευρήματα των Ντάνινγκ–Κρούγκερ εντοπίζοντας μεθοδολογικά λάθη και υποστηρίζουν ότι όλοι υπερεκτιμούν τις ικανότητές τους.
Δεν παραθέτω τις πηγές μου, αν δεν σου αρέσει πήγαινε να διαβάσεις το Νature. Πάντως το τελευταίο συμπέρασμα με βολεύει και θα το κρατήσω για τη συνέχεια.&lt;/p&gt;
&lt;p&gt;Κλείνοντας θα ήθελα να σημειώσω ότι στους Ντάνινγκ–Κρούγκερ απονεμήθηκε το σατιρικό Ig Νόμπελ ψυχολογίας.
Δηλαδή οι Ντάνινγκ–Κρούγκερ μάλλον είχαν υπερεκτιμήσει την επιστημονική αξία του έργου τους. Φοβερό; Μόνος μου το σκέφτηκα.&lt;/p&gt;
&lt;h2 id=&#34;o-elon-musk&#34;&gt;O Elon Musk&lt;/h2&gt;
&lt;p&gt;Ομολογώ ότι δυσκολεύτηκα να αποφασίσω για το εαν θα έπρεπε να γράψω αυτά που ακολουθούν. Άρχικα με τον Elon μας συνδέει μια βαθιά φιλιά και εκτίμηση. Μπορεί να μην
με ξέρει αλλά η καταγωγή μας είναι κοινή. Όχι, ο Elon δεν είναι Πυργιώτης αλλά γεννήθηκε και μεγάλωσε στην Νότια Αφρική. Είναι γνωστό επίσης ότι και ο Homo
sapiens έζησε στην Αφρική πριν απο 200.000 χρόνια. Αυτό.&lt;/p&gt;
&lt;p&gt;Ο σημαντικότερος όμως ανασταλτικός παράγοντας για να ασκήσω κριτίκη στον Elon είναι οτι κάποια στιγμή στο μέλλον έχω δουλέψει σε μία απο τις εταιρίες του. Σε περίπτωση που η
προηγούμενη πρόταση δεν ακούγεται σωστή είναι γιατί αν θέλεις να συμβεί κάτι τότε το σύμπαν συνομώτει για να τα καταφέρεις. Αντίστροφα, αν δεν τα καταφέρεις είναι γιατί δεν συνομώτησε.
Τώρα αν ο Elon διαβάσει αυτό το άρθρο ενδέχεται να με αντιπαθήσει αλλά υποθέτοντας οτι τον πετυχαίνω στην ανάγκη, πιστεύω ότι θα αναγκαστεί να με προσλάβει στο μαγαζί του.
Το έχω δει να συμβαίνει live σε σουβλατζίδικο όταν ο ψήστης τσακώθηκε με το αφεντικό, παράτησε τα μπιφτέκια στη μέση αλλά μετά απο λίγο επέστρεψε για να συνεχίσει το ψήσιμο. Αν το καλοσκεφτείς και η Tesla ένα μεγάλο
σουβλατζίδικο είναι.&lt;/p&gt;
&lt;p&gt;Όσοι απο τους αναγνώστες μου έζησαν τα εφηβικά τους χρόνια στα 90s σίγουρα θα θυμούνται τον Τζίμη τον Αλήθεια. Μια απο τις πλέον χαρακτηριστικές ιστορίες του είναι αυτή με τη μεγάλη φωτιά στο χωριό του. Κάποια στιγμή το πυροσβεστικό αεροπλάνο επιστρέφει για ανεφοδιασμό. Τότε μαζί με το νερό, παίρνει και έναν καρχαρία ο οποίος έτυχε να κολυμπάει στην περιοχή υδροληψίας. Ρίχνοντας το νερό στο φλεγόμενο χωριό, &lt;strong&gt;ο καρχαρίας προσγειώνεται στο έδαφος και τρώει τα πρόβατα του παππού του&lt;/strong&gt;. Αν διαβάσει κανείς το Quora θα βρεί ιστορίες για τον Elon οι οποίες κάνουν το προηγούμενο σενάριο να μοιάζει σαν ένα αριστούργημα του σοσιαλιστικού ρεαλισμού. Μαθαίνουμε ότι υπήρξε παιδί θαυμα και μέχρι 2 ετών είχε διαβάσει ότι κυκλοφορούσε, δηλαδή απο τον κώδικα του Χαμουραμπί μέχρι κβαντική φυσική. Όταν τα υπόλοιπα παιδάκια έβλεπαν Thundercats αυτός διάβαζε Σιμόν ντε Μπoβουάρ. Αφου τα έμαθε όλα, άρχισε να εφαρμόζει τις γνώσεις του. Έφτιαξε απο μόνος του ένα video game, ένα μίξερ και ένα πολυόργανο Ηρακλής. Πήγε στο Stanford αλλά το παράτησε γιατί άκουσε το κάλεσμα απο τον ουρανό. Έπρεπε να σώσει την ανθρωπότητα. Δεν κοιμάται ποτέ, ίσως δεν τρώει. Ένας γραφικός Ινδός τον χαρακτήρισε ως metahuman. Τρέχει ταυτόχρονα 5000 projects και 10 εταιρείες. Φτιάχνει μπαταρίες, πυραύλους, είναι κοντά στην AI singularity και ετοιμάζεται να πεθάνει στον Άρη. Δεν έχει νόημα να συνεχίσω. Αυτός δεν είναι άνθρωπος. Το είπε εξάλλου ο Ινδός.&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://georgedouzas.github.io/blog/blog/post/dunning-krugger/elon-musk_hudee8bd004f44aac87b0a70b4f971b441_403016_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Αυτός ο τύπος θα σώσει την ανθρωπότητα. Μια απλή καράφλα δεν είναι αρκετή για να τον σταματήσει.&#34;&gt;


  &lt;img data-src=&#34;https://georgedouzas.github.io/blog/blog/post/dunning-krugger/elon-musk_hudee8bd004f44aac87b0a70b4f971b441_403016_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;700px&#34; height=&#34;403&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Αυτός ο τύπος θα σώσει την ανθρωπότητα. Μια απλή καράφλα δεν είναι αρκετή για να τον σταματήσει.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Όταν οι διάφοροι θαυμαστές του επαναλαμβάνουν συνεχώς τα παραπάνω με όλο και αυξανόμενη ένταση τότε είναι λογίκο να τα πιστέψει. Εδώ έχεις τη μάνα σου να λέει ότι είσαι το πιο όμορφο παιδάκι και τελικά το πιστεύεις, σκέψου να στο λένε άγνωστοι. Το αποτέλεσμα είναι αυτή η αηδία, η γελοιότητα, ο πιο εκνευριστικός κλαρινογαμπρός-εντερπρενέρ στο κόσμο, ο Elon Musk.&lt;/p&gt;
&lt;h2 id=&#34;μαθηματικό-μοντέλο&#34;&gt;Μαθηματικό μοντέλο&lt;/h2&gt;
&lt;p&gt;Ο Elon Musk λοιπόν είναι άλλος ένας άνθρωπος ο οποίος συστηματικά υπερεκτιμάει τις ικανότητές του. Προκλητικά αγνοεί τον παράγοντα τύχη και θεωρεί ότι είναι κλώνος του Νίκολα Τέσλα. Νομίζω ότι τώρα κάπως το έσωσα και το κούμπωσα με το φαινόμενο Ντάνινγκ–Κρούγκερ. Ευτυχώς γιατί είχα αρχίσει να ανησυχώ ότι θα χάσω το κοινό μου. Δεν είναι εκεί το θέμα όμως. Σκοπός μου είναι να δείξω πως με απλά μαθηματικά μπορεί κανείς να ποσοτικοποιήσει, σχεδόν να εξηγήσει, όλα τα παραπάνω.&lt;/p&gt;
&lt;p&gt;Πρίν μπώ δυνατά στις λεπτομέρειες και στο φορμαλισμό του μαθηματικού μοντέλου θα ήθελα να κάνω μια σύντομη εισαγωγή. Πιο συγκεκριμένα θα προσπαθήσω να περιγραψω πως οδηγήθηκα σε αυτό γιατί σύχνα εμείς οι επιστήμονες δίνουμε την εντύπωση ότι οι ιδέες μας γεννιούνται στο κενό. Ένα ερώτημα το οποίο με βασάνιζε απο 2.5 μηνών ήταν ποιό στοιχείο διαφοροποιεί τον άνθρωπο απο τα ζώα. Δεν ισχυρίζομαι ότι είμαι ο πρώτος που το σκέφτηκε. Ας είμαστε σοβαροί. Όμως είμαι ο πρώτος που το έλυσε και μάλιστα πριν απο 40 χρόνια:&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Η θεμελιώδης διαφορά ανάμεσα στον άνθρωπο και στα ζώα (λιοντάρια, κουνέλια, κόμπρες, άλογα κτλ) είναι η παπάτζα.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Έχεις δει ποτέ ζώο να πουλάει παραμύθι; Θα προσπαθήσει ποτέ η αρκούδα να σε πείσει ότι μπορεί να σε φάει; Η χελώνα θα σου εξηγήσει με επιχειρήματα ότι είναι πιο γρήγορη απο εσένα; Κατάλαβες που το πάω; Αν όχι, δεν πειράζει. Οι μεγάλες ιδέες δεν γίνονται άμεσα κατανοητές.&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://georgedouzas.github.io/blog/blog/post/dunning-krugger/priest_huef45f6c9876e80cf771040bddc4db553_208997_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Ο Elon Musk σε μια βραδινή έξοδο στο Lohan Nightclub.&#34;&gt;


  &lt;img data-src=&#34;https://georgedouzas.github.io/blog/blog/post/dunning-krugger/priest_huef45f6c9876e80cf771040bddc4db553_208997_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;700px&#34; height=&#34;445&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Ο Elon Musk σε μια βραδινή έξοδο στο Lohan Nightclub.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Το μοντέλο μου λοιπόν ξεκινάει με την υπόθεση ότι η μόνη πραγματική δεξιότητα που υπάρχει στον άνθρωπο είναι η παπάτζα. Ομολογώ ότι είναι ισχυρή υπόθεση αλλά όπως είναι γνωστό:&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    All models are wrong but some are useful.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Το γράφω στα αγγλικά γιατί το ίδιο στα ελληνικά δεν γράφει ωραία. Δίχως να χάνω χρόνο, προχωρώ στο notation. Ονομάζω ως $PF$ (Priest Factor) τον παράγοντα ο οποίος μετράει την ικανότητα ενός ατόμου στη παπάτζα και θεωρώ ότι παίρνει τιμές στο σύνολο των πραγματικών αριθμών. Υποθέτω ότι η αυτοπεποίθηση ενός ατόμου, την οποία ονομάζω $CF$ (Confidence Factor), εξαρτάται απο την αλληλεπίδρασή του με κάποια άλλα άτομα. Αυτό είναι λογικό αν το σκεφτείς. Πάντα συγκρίνεις τον εαυτό σου με τους άλλους. Είσαι κοντός γιατί οι άλλοι είναι ψηλοί. Είσαι όμορφος γιατί οι άλλοι είναι μπάζα κ.ο.κ. Στη ζωή όλα είναι σχετικά σύμφωνα με τον Einstein (ή τη γυναίκα του αν πιστέψουμε τις διάφορες θωρίες συνομωσίας).&lt;/p&gt;
&lt;p&gt;Το κρίσιμο σημείο είναι ότι ο $CF$ ενός ατόμου είναι συνάρτηση του $PF$ του ίδιου ατόμου καθώς και των $PF_{i}$ των $i = 1, \cdots, n$ ατόμων με τα οποία αλληλεπιδρά. Ποιοτικά θα λέγαμε ότι αν βρέθεις σε μια παρέα στην οποία πουλάς τη μεγαλύτερη παπάτζα τότε όλοι σε πιστεύουν και η αυτοπεποίθησή σου πιάνει μέγιστο. Ομοίως αν είσαι ο ηλίθιος της παρέας, ή ισοδύναμα αυτός που έχει τη μικρότερη ικανότητα στο παραμύθι, τότε κανένας δεν σε πιστεύει και επειδή δεν το παίρνεις καν χαμπάρι (αφού οι υπόλοιποι σου πούλησαν το δικό τους ψέμα) πάλι έχεις μεγάλη σιγουριά για τον εαυτό σου. Το πρόβλημα εμφανίζεται όταν βρίσκονται στον ίδιο χώρο άτομα με αντίστοιχα επίπεδα παπάτζας. Τότε στην περίπτωση που είναι και οι δύο καλοί ψεύτες είναι αδύνατο να κοροιδέψει ο ένας τον άλλον. Αντίστοιχα εάν κανένας απο τους δύο δεν διαθέτει αυτή την ικανότητα τότε μένουν αποσβωλομένοι να κοιτούν με το χαρακτηριστικό βλέμα του πιγκουίνου περιμένοντας κάποιος να πεί το πρώτο ψέμα. Το αποτέλεσμα και στα δύο σενάρια είναι η χαμηλή αυτοπεποίθηση. Ποσοτικά η παραπάνω περιγραφή αντιστοιχεί στην εξής σχέση:&lt;/p&gt;
&lt;p&gt;$$CF = \frac{1}{n} \displaystyle\sum_{i=1}^n | PF - PF_{i} |$$&lt;/p&gt;
&lt;p&gt;δηλαδή η αυτοπεποίθηση ενός ατόμου είναι ίση με το άθροισμα των αποστάσεων ανάμεσα στη παπάτζα του και στην παπάτζα των υπολοίπων. Αυτό είναι πραγματικά deep result. Very deep.&lt;/p&gt;
&lt;p&gt;Υποθέτω δίχως δισταγμό ότι ο $PF_{i}$ για οποιοδήποτε απο αυτά τα ανεξάρτητα άτομα ακολουθεί μια τυποποιημένη κανονική κατανομή, δηλαδή:&lt;/p&gt;
&lt;p&gt;$$PF_{i} \stackrel{i.i.d.}{\sim} N(0, 1)$$&lt;/p&gt;
&lt;p&gt;Η ποσότητα που μας ενδιαφέρει είναι η αναμενόμενη τιμή του $CF$. Απο τις δύο προηγούμενες σχέσεις προκύπτει ότι ο $CF$ ακολουθεί τη λεγόμενη ως διπλωμένη κανονική κατανομή και συνεπώς:&lt;/p&gt;
&lt;p&gt;$E[CF] = \frac{2}{\pi} exp \Big( - \frac{PF}{2} \Big) - PF \cdot erf \Big( - \frac{PF}{\sqrt{2}} \Big)$&lt;/p&gt;
&lt;p&gt;όπου $erf$ η 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Error_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;συνάρτηση σφάλματος&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Θα σχεδιάσω την αναμενόμενη τιμή $E[CF]$ ως συνάρτηση του $PF$ για τιμές γύρω απο το μηδεν και τρείς τυπικές αποκλίσεις. Το παρακάτω απόσπασμα κώδικα σε Python 3 παράγει το ζητούμενο γράφημα:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
from math import sqrt, exp, erf
import numpy as np
import matplotlib.pyplot as plt

# Define expectation of confidence factors
def calculate_confidence_factors(priest_factors):
    values = (2.0 / np.pi) * np.exp(-priest*factors / 2)
    corrections = priest_factors * np.erf(-priest_factors / np.sqrt(2))
    return values - corrections

# Generate priest and confidence factors values
priest_factors = np.linspace(-3.0, 3.0, 20)
expected_confidence_factors = calculate_confidence_factors(priest_factors)

# Plot
fig, ax = plt.subplots()
ax.plot(priest_factors, expected_confidence_factors)
ax.set(
    xlabel=&#39;Priest Factor&#39;,
    ylabel=&#39;Expected Confidence Factor&#39;,
    title=&#39;Dunning-Kruger Effect&#39;
)
ax.grid()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Η ομοιότητα με το υποτιθέμενο Ντάνινγκ–Κρούγκερ γράφημα είναι χωρίς υπερβολές ανατριχιαστική. Απο την συγκίνηση δυσκολέυομαι να γράψω ακόμα και τον επίλογο αυτού του άρθρου. Θα χαρακτήριζα την ανακάλυψη του μοντέλου ως μια μεταφυσική εμπειρία η οποία με άλλαξε ως άνθρωπο. Συγκεκριμένα μου προκάλεσε μια αστρική προβολή η οποία με ταξίδεψε στα όρια του γαλαξία μας. Ας αφήσω καλύτερα τις λεπτομέρειες για ένα επόμενο άρθρο.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Improving imbalanced learning through a heuristic oversampling method based on k-means and SMOTE</title>
      <link>https://georgedouzas.github.io/blog/publication/kmeans_smote_journal/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://georgedouzas.github.io/blog/publication/kmeans_smote_journal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Effective data generation for imbalanced learning using conditional generative adversarial networks</title>
      <link>https://georgedouzas.github.io/blog/publication/cgan/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://georgedouzas.github.io/blog/publication/cgan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Oversampling for Imbalanced Learning Based on K-Means and SMOTE</title>
      <link>https://georgedouzas.github.io/blog/publication/kmeans_smote_preprint/</link>
      <pubDate>Tue, 12 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://georgedouzas.github.io/blog/publication/kmeans_smote_preprint/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self-Organizing Map Oversampling (SOMO) for imbalanced data set learning</title>
      <link>https://georgedouzas.github.io/blog/publication/somo/</link>
      <pubDate>Sun, 01 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://georgedouzas.github.io/blog/publication/somo/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Geometric SMOTE: Effective oversampling for imbalanced learning through a geometric extension of SMOTE</title>
      <link>https://georgedouzas.github.io/blog/publication/gsmote_preprint/</link>
      <pubDate>Thu, 21 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://georgedouzas.github.io/blog/publication/gsmote_preprint/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Coset space dimensional reduction and Wilson flux breaking of ten-dimensional N=1, E(8) gauge theory</title>
      <link>https://georgedouzas.github.io/blog/publication/csdr2/</link>
      <pubDate>Fri, 12 Dec 2008 00:00:00 +0000</pubDate>
      <guid>https://georgedouzas.github.io/blog/publication/csdr2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Coset space dimensional reduction and classification of semi‐realistic particle physics models</title>
      <link>https://georgedouzas.github.io/blog/publication/csdr/</link>
      <pubDate>Tue, 08 Apr 2008 00:00:00 +0000</pubDate>
      <guid>https://georgedouzas.github.io/blog/publication/csdr/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
