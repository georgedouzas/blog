<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Geometric SMOTE | AI and friends</title>
    <link>https://georgedouzas.github.io/site/tags/geometric-smote/</link>
      <atom:link href="https://georgedouzas.github.io/site/tags/geometric-smote/index.xml" rel="self" type="application/rss+xml" />
    <description>Geometric SMOTE</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 17 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Geometric SMOTE</title>
      <link>https://georgedouzas.github.io/site/tags/geometric-smote/</link>
    </image>
    
    <item>
      <title>Imbalanced Learning in Land Cover Classification: Improving Minority Classesâ€™ Prediction Accuracy Using the Geometric SMOTE Algorithm</title>
      <link>https://georgedouzas.github.io/site/publication/land_cover_classification/</link>
      <pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://georgedouzas.github.io/site/publication/land_cover_classification/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Geometric SMOTE a geometrically enhanced drop-in replacement for SMOTE</title>
      <link>https://georgedouzas.github.io/site/publication/gsmote_journal/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://georgedouzas.github.io/site/publication/gsmote_journal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Geometric SMOTE</title>
      <link>https://georgedouzas.github.io/site/project/geometric-smote/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://georgedouzas.github.io/site/project/geometric-smote/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Classification of imbalanced datasets is a challenging task for standard machine learning algorithms. Training a classifier on imbalanced data, often results in a low out-of-sample accuracy for the minority classes. To deal with this problem several approaches have been proposed. A general approach, known as oversampling, is the generation of artificial data for the minority classes that are used to enhance the training data.&lt;/p&gt;
&lt;p&gt;SMOTE is the most popular oversampling algorithm, while many variants of it have been developed. SMOTE generates synthetic data between the line segment that connects two randomly chosen neighboring minority class instances. On the other hand, Geometric SMOTE expands the data generation area by generating synthetic data inside a hypersphere that is defined by a randomly chosen minority class instance and one of its neighbors either from the minority or majority class. Geometric SMOTE 
&lt;a href=&#34;../../publication/gsmote_journal&#34;&gt;has been shown&lt;/a&gt; to outperform other standard oversamplers in a large number of datasets. The following figure illustrates the difference between the two data generation mechanisms:&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://georgedouzas.github.io/site/site/project/geometric-smote/smote_vs_gsmote_huf5fcb04c55734e9a4ddd9006d03d5b15_12117_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;SMOTE vs Geometric SMOTE&#34;&gt;


  &lt;img data-src=&#34;https://georgedouzas.github.io/site/site/project/geometric-smote/smote_vs_gsmote_huf5fcb04c55734e9a4ddd9006d03d5b15_12117_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;700px&#34; height=&#34;540&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    SMOTE vs Geometric SMOTE
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;A Python implementation of SMOTE and several of its variants is available in the 
&lt;a href=&#34;https://imbalanced-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbalanced-Learn&lt;/a&gt; library, which is fully compatible with the popular machine learning toolbox 
&lt;a href=&#34;https://scikit-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scikit-Learn&lt;/a&gt;. I have developed a Python 
&lt;a href=&#34;https://github.com/AlgoWit/geometric-smote&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;implementation&lt;/a&gt; of Geometric SMOTE oversampler, called &lt;code&gt;geometric-smote&lt;/code&gt;, that integrates seamlessly with the Scikit-Learn and Imbalanced-Learn ecosystems.&lt;/p&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;
&lt;p&gt;The easiest way to install the &lt;code&gt;geometric-smote&lt;/code&gt; package, assuming you have already Python 3 and &lt;code&gt;pip&lt;/code&gt; installed as well as you have optionally activated a Python virtual environment, is to open a shell and run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install geometric-smote
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will install the latest version and its dependencies.&lt;/p&gt;
&lt;h2 id=&#34;documentation&#34;&gt;Documentation&lt;/h2&gt;
&lt;p&gt;Detailed documentation that includes installation guidelines, API description and various examples can found 
&lt;a href=&#34;https://geometric-smote.readthedocs.io/en/latest/?badge=latest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;functionality&#34;&gt;Functionality&lt;/h2&gt;
&lt;p&gt;In what follows, I will describe briefly some aspects of &lt;code&gt;geometric-smote&lt;/code&gt;&#39;s functionality. The class that represents the Geometric SMOTE oversampler is called &lt;code&gt;GeometricSMOTE&lt;/code&gt;. Its API follows closely the API of oversamplers provided by Imbalanced-Learn.&lt;/p&gt;
&lt;h4 id=&#34;resampling-an-imbalanced-dataset&#34;&gt;Resampling an imbalanced dataset&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s generate a binary class imbalanced dataset, represented by the input matrix &lt;code&gt;X&lt;/code&gt; and the target vector &lt;code&gt;y&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
from sklearn.datasets import make_classification

# Set random seed
rnd_seed = 43

# Generate imbalanced data
X, y = make_classification(
  n_samples=100,
  n_classes=2,
  weights=[0.9, 0.1],
  random_state=rnd_seed
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following functions extract and print the main characteristics of a binary class dataset. Specifically, the &lt;code&gt;extract_characteristics&lt;/code&gt; function returns the number of samples, the number of features,  the labels and the number of samples for the majority and minority classes as well as the Imbalance Ratio defined as the ratio between the number of samples of the majority and minority classes, while the &lt;code&gt;print_characteristics&lt;/code&gt; function prints them in an appropriate format:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
from collections import Counter

# Define function to extract dataset&#39;s characteristics
def extract_characteristics(X, y):
  n_samples, n_features = X.shape
  count_y = Counter(y)
  (maj_label, n_samples_maj), (min_label, n_samples_min) = count_y.most_common()
  ir = n_samples_maj / n_samples_min
  return n_samples, n_features, maj_label, n_samples_maj, min_label, n_samples_min, ir

# Define function to print dataset&#39;s characteristics
def print_characteristics(X, y):
  n_samples, n_features, maj_label, n_samples_maj, min_label, n_samples_min, ir = extract_characteristics(X, y)
  print(
    f&#39;Number of samples: {n_samples}&#39;,
    f&#39;Number of features: {n_features}&#39;,
    f&#39;Majority class label: {maj_label}&#39;,
    f&#39;Number of majority class samples: {n_samples_maj}&#39;,
    f&#39;Minority class label: {min_label}&#39;,
    f&#39;Number of minority class samples: {n_samples_min}&#39;,
    f&#39;Imbalance Ratio: {ir:.1f}&#39;,
    sep=&#39;\n&#39;
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I use the above function to print the main characteristics of the generated imbalanced dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print imbalanced dataset&#39;s characteristics
print_characteristics(X, y)

##########
# Output #
##########

# Number of samples: 100
# Number of features: 20
# Majority class label: 0
# Number of majority class samples: 90
# Minority class label: 1
# Number of minority class samples: 10
# Imbalance Ratio: 9.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Following the Imbalanced Learn&amp;rsquo;s API, the &lt;code&gt;fit_resample&lt;/code&gt; method of a &lt;code&gt;GeometricSMOTE&lt;/code&gt; instance can be used to resample the imbalanced dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
from gsmote import GeometricSMOTE

# Create GeometricSMOTE instance
geometric_smote = GeometricSMOTE(random_state=rnd_seed + 5)

# Fit and resample imbalanced data
X_res, y_res = geometric_smote.fit_resample(X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again we can print the main characteristics of the rebalanced dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print balanced dataset&#39;s characteristics
print_characteristics(X_res, y_res)

##########
# Output #
##########

# Number of samples: 180
# Number of features: 20
# Majority class label: 0
# Number of majority class samples: 90
# Minority class label: 1
# Number of minority class samples: 90
# Imbalance Ratio: 1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, the default behavior of the &lt;code&gt;GeometricSMOTE&lt;/code&gt; instance is to generate the appropriate number of minority class instances so that the resampled dataset is perfectly balanced.&lt;/p&gt;
&lt;h4 id=&#34;performance-on-out-of-sample-data&#34;&gt;Performance on out-of-sample data&lt;/h4&gt;
&lt;p&gt;As I mentioned above, training a classifier on imbalanced data may result in suboptimal performance on out-of-sample data. The function &lt;code&gt;calculate_cv_scores&lt;/code&gt; calculates the average 10-fold cross-validation geometric mean and accuracy scores across 100 runs of a decision tree classifier that is optionally combined to an oversampler through a pipeline:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_validate, StratifiedKFold
from sklearn.metrics import make_scorer, SCORERS
from imblearn.pipeline import make_pipeline
from imblearn.metrics import geometric_mean_score

# Append geometric mean score
SCORERS[&#39;g_mean&#39;] = make_scorer(geometric_mean_score)

# Define function that calculates out-of-sample scores
def calculate_cv_scores(oversampler, X, y):
  mean_cv_scores= []
  scoring = [&#39;g_mean&#39;, &#39;accuracy&#39;]
  n_runs = 100
  for ind in range(n_runs):
    rnd_seed = 10 * ind
    classifier = DecisionTreeClassifier(random_state=rnd_seed)
    if oversampler is not None:
      classifier = make_pipeline(
        oversampler.set_params(random_state=rnd_seed + 4), 
        classifier
      )
    cv_scores = cross_validate(
      estimator=classifier,
      X=X,
      y=y,
      scoring=scoring,
      cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=rnd_seed + 6)
    )
    cv_scores = [cv_scores[f&#39;test_{scorer}&#39;].mean() for scorer in scoring]
    mean_cv_scores.append(cv_scores)
  return cv_scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the above function we can calculate the out-of-sample performance when no oversampling is applied as well as when SMOTE and Geometric SMOTE are used as oversamplers:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
from imblearn.over_sampling import SMOTE

# Calculate cross-validation scores
mapping = {&#39;No oversampling&#39;: None, &#39;SMOTE&#39;: SMOTE(), &#39;Geometric SMOTE&#39;: GeometricSMOTE()}
cv_scores = {}
for name, oversampler in mapping.items():
  cv_scores[name] = calculate_cv_scores(oversampler, X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Printing a table of the scores, we see that Geometric SMOTE outperforms the other methods when geometric mean score is used as an evaluation metric, while the highest accuracy is achieved when no oversampling is applied:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cv_scores = pd.DataFrame(cv_scores, index = [&#39;Geometric Mean&#39;, &#39;Accuracy&#39;])
print(cv_scores)

##########
# Output #
##########

#                 No oversampling     SMOTE  Geometric SMOTE
# Geometric Mean         0.782843  0.582843         0.841616
# Accuracy               0.950000  0.920000         0.870000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that using the accuracy as an evaluation metric is not considered a good choice when the data is imbalanced. For example, a trivial classifier that always predicts the majority class would still have an accuracy equal to 0.90, even though all the minority class instances are misclassified. On the other hand, geometric mean score is an appropriate evaluation metric for imbalanced data since it equally weighs the accuracies per class.&lt;/p&gt;
&lt;p&gt;For more details you can look at the &lt;code&gt;geometric-smote&lt;/code&gt; 
&lt;a href=&#34;https://geometric-smote.readthedocs.io/en/latest/?badge=latest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;. The 
&lt;a href=&#34;https://imbalanced-learn.readthedocs.io/en/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; of the &lt;code&gt;imbalanced-learn&lt;/code&gt; project provides also various examples and an introduction to the imbalanced learning problem.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Geometric SMOTE: Effective oversampling for imbalanced learning through a geometric extension of SMOTE</title>
      <link>https://georgedouzas.github.io/site/publication/gsmote_preprint/</link>
      <pubDate>Thu, 21 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://georgedouzas.github.io/site/publication/gsmote_preprint/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
