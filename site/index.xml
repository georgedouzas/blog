<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI and friends</title>
    <link>https://aiandfriends.blog/site/</link>
      <atom:link href="https://aiandfriends.blog/site/index.xml" rel="self" type="application/rss+xml" />
    <description>AI and friends</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 17 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>AI and friends</title>
      <link>https://aiandfriends.blog/site/</link>
    </image>
    
    <item>
      <title>Handstand</title>
      <link>https://aiandfriends.blog/site/hobbies/gymnastics/handstand/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://aiandfriends.blog/site/hobbies/gymnastics/handstand/</guid>
      <description>&lt;h2 id=&#34;pike-press&#34;&gt;Pike press&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/nrX3ExQflu0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;l-sit-to-press-handstand&#34;&gt;L-sit to press handstand&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/4stgsUEla0o&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;kneeling-jump-to-handstand&#34;&gt;Kneeling jump to handstand&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Tb5IXQAJMPI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Planche</title>
      <link>https://aiandfriends.blog/site/hobbies/gymnastics/planche/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://aiandfriends.blog/site/hobbies/gymnastics/planche/</guid>
      <description>&lt;h2 id=&#34;straddle-planche&#34;&gt;Straddle planche&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/cPh7iK6plpo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Imbalanced Learning in Land Cover Classification: Improving Minority Classes’ Prediction Accuracy Using the Geometric SMOTE Algorithm</title>
      <link>https://aiandfriends.blog/site/publication/land_cover_classification/</link>
      <pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/publication/land_cover_classification/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Geometric SMOTE a geometrically enhanced drop-in replacement for SMOTE</title>
      <link>https://aiandfriends.blog/site/publication/gsmote_journal/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/publication/gsmote_journal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Clustering-Based Oversampling</title>
      <link>https://aiandfriends.blog/site/project/clustering-based-oversampling/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/project/clustering-based-oversampling/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Learning from imbalanced data is a common and challenging problem in supervised learning. Training a classifier on imbalanced data, often results in a low out-of-sample accuracy for the minority classes. While different strategies exist to tackle this problem, the most general approach known as oversampling, is the generation of artificial data to achieve a balanced class distribution that in turn are used to enhance the training data.&lt;/p&gt;
&lt;p&gt;SMOTE algorithm, the most popular oversampler, as well as any other oversampling method based on it, generates synthetic samples along line segments that join minority class instances. SMOTE addresses only the issue of between-classes imbalance. On the other hand, by clustering the input space and applying any oversampling algorithm for each resulting cluster with appropriate resampling ratio, the within-classes imbalanced issue can be addressed i.e. areas of the input space that differ significantly in the density of the minority class. 
&lt;a href=&#34;../../publication/somo&#34;&gt;SOMO&lt;/a&gt; and 
&lt;a href=&#34;../../publication/kmeans_smote_journal&#34;&gt;KMeans-SMOTE&lt;/a&gt; are specific realizations of this approach that have been shown to outperform other standard oversamplers in a large number of datasets.&lt;/p&gt;
&lt;p&gt;A Python implementation of SMOTE and several of its variants is available in the 
&lt;a href=&#34;https://imbalanced-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbalanced-Learn&lt;/a&gt; library, which is fully compatible with the popular machine learning toolbox 
&lt;a href=&#34;https://scikit-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scikit-Learn&lt;/a&gt;. I have developed a Python 
&lt;a href=&#34;https://github.com/AlgoWit/cluster-over-sampling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;implementation&lt;/a&gt; of the above clustering-based oversampling approach, called &lt;code&gt;cluster-over-sampling&lt;/code&gt;, that integrates seamlessly with the Scikit-Learn and Imbalanced-Learn ecosystems.&lt;/p&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;
&lt;p&gt;The easiest way to install the &lt;code&gt;cluster-over-sampling&lt;/code&gt; package, assuming you have already Python 3 and &lt;code&gt;pip&lt;/code&gt; installed as well as you have optionally activated a Python virtual environment, is to open a shell and run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install cluster-over-sampling
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will install the latest version and its dependencies.&lt;/p&gt;
&lt;h2 id=&#34;documentation&#34;&gt;Documentation&lt;/h2&gt;
&lt;p&gt;Detailed documentation that includes installation guidelines, API description and various examples can found 
&lt;a href=&#34;https://cluster-over-sampling.readthedocs.io/en/latest/?badge=latest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;functionality&#34;&gt;Functionality&lt;/h2&gt;
&lt;p&gt;In what follows, I will describe briefly some aspects of &lt;code&gt;cluster-over-sampling&lt;/code&gt;&#39;s functionality. The main modification that &lt;code&gt;cluster-over-sampling&lt;/code&gt; applies to the oversamplers provided by Imbalanced-Learn is that it appends to their initializer the &lt;code&gt;clusterer&lt;/code&gt; and the &lt;code&gt;distributor&lt;/code&gt; parameters. The first parameter defines the clustering algorithm that is applied to the input space before oversampling. It can be any Scikit-Learn compatible clusterer. The second parameter should be a distributor object i.e. an instance of any class that inherits from the &lt;code&gt;BaseDistributor&lt;/code&gt; class provided by &lt;code&gt;cluster-over-sampling&lt;/code&gt;. Such a class is already included and it is called &lt;code&gt;DensityDistributor&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;resampling-an-imbalanced-dataset&#34;&gt;Resampling an imbalanced dataset&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s generate a binary class imbalanced dataset, represented by the input matrix &lt;code&gt;X&lt;/code&gt; and the target vector &lt;code&gt;y&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
from sklearn.datasets import make_classification

# Set random seed
rnd_seed = 12

# Generate imbalanced data
X, y = make_classification(
  n_samples=100,
  n_classes=2,
  weights=[0.9, 0.1],
  random_state=rnd_seed
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following functions extract and print the main characteristics of a binary class dataset. Specifically, the &lt;code&gt;extract_characteristics&lt;/code&gt; function returns the number of samples, the number of features, the labels and the number of samples for the majority and minority classes as well as the Imbalance Ratio defined as the ratio between the number of samples of the majority and minority classes, while the &lt;code&gt;print_characteristics&lt;/code&gt; function prints them in an appropriate format:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
from collections import Counter

# Define function to extract dataset&#39;s characteristics
def extract_characteristics(X, y):
  n_samples, n_features = X.shape
  count_y = Counter(y)
  (maj_label, n_samples_maj), (min_label, n_samples_min) = count_y.most_common()
  ir = n_samples_maj / n_samples_min
  return n_samples, n_features, maj_label, n_samples_maj, min_label, n_samples_min, ir

# Define function to print dataset&#39;s characteristics
def print_characteristics(X, y):
  n_samples, n_features, maj_label, n_samples_maj, min_label, n_samples_min, ir = extract_characteristics(X, y)
  print(
    f&#39;Number of samples: {n_samples}&#39;,
    f&#39;Number of features: {n_features}&#39;,
    f&#39;Majority class label: {maj_label}&#39;,
    f&#39;Number of majority class samples: {n_samples_maj}&#39;,
    f&#39;Minority class label: {min_label}&#39;,
    f&#39;Number of minority class samples: {n_samples_min}&#39;,
    f&#39;Imbalance Ratio: {ir:.1f}&#39;,
    sep=&#39;\n&#39;
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I use the above function to print the main characteristics of the generated imbalanced dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print imbalanced dataset&#39;s characteristics
print_characteristics(X, y)

##########
# Output #
##########

# Number of samples: 100
# Number of features: 20
# Majority class label: 0
# Number of majority class samples: 90
# Minority class label: 1
# Number of minority class samples: 10
# Imbalance Ratio: 9.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will use the KMeans-SMOTE algorithm to rebalance the above dataset. An instance of KMeans-SMOTE can be constructed by importing &lt;code&gt;SMOTE&lt;/code&gt; oversampler from &lt;code&gt;cluster-over-sampling&lt;/code&gt; instead of Imbalanced-Learn and setting the &lt;code&gt;clusterer&lt;/code&gt; parameter equal to an instance of the &lt;code&gt;KMeans&lt;/code&gt; clusterer. Then following the Imbalanced Learn&amp;rsquo;s API, the &lt;code&gt;fit_resample&lt;/code&gt; method of the resulting oversampler can be used to resample the imbalanced dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
from clover.over_sampling import SMOTE
from clover.distribution import DensityDistributor
from sklearn.cluster import KMeans

# Create KMeans-SMOTE instance
kmeans = KMeans(n_clusters=50, random_state=rnd_seed + 2)
kmeans_smote = SMOTE(clusterer=kmeans, random_state=rnd_seed + 5)

# Fit and resample imbalanced data
X_res, y_res = kmeans_smote.fit_resample(X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again we can print the main characteristics of the rebalanced dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print balanced dataset&#39;s characteristics
print_characteristics(X_res, y_res)

##########
# Output #
##########

# Number of samples: 180
# Number of features: 20
# Majority class label: 0
# Number of majority class samples: 90
# Minority class label: 1
# Number of minority class samples: 90
# Imbalance Ratio: 1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, the default behavior of the oversampler is to generate the appropriate number of minority class instances so that the resampled dataset is perfectly balanced.&lt;/p&gt;
&lt;h4 id=&#34;performance-on-out-of-sample-data&#34;&gt;Performance on out-of-sample data&lt;/h4&gt;
&lt;p&gt;As I mentioned above, training a classifier on imbalanced data may result in suboptimal performance on out-of-sample data. The function &lt;code&gt;calculate_cv_scores&lt;/code&gt; calculates the average 10-fold cross-validation geometric mean and accuracy scores across 100 runs of a decision tree classifier that is optionally combined to an oversampler through a pipeline:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_validate, StratifiedKFold
from sklearn.metrics import make_scorer, SCORERS
from imblearn.pipeline import make_pipeline
from imblearn.metrics import geometric_mean_score

# Append geometric mean score
SCORERS[&#39;g_mean&#39;] = make_scorer(geometric_mean_score)

# Define function that calculates out-of-sample scores
def calculate_cv_scores(oversampler, X, y):
  mean_cv_scores= []
  scoring = [&#39;g_mean&#39;, &#39;accuracy&#39;]
  n_runs = 100
  for ind in range(n_runs):
    rnd_seed = 10 * ind
    classifier = DecisionTreeClassifier(random_state=rnd_seed)
    if oversampler is not None:
      classifier = make_pipeline(
        oversampler.set_params(random_state=rnd_seed + 4), 
        classifier
      )
    cv_scores = cross_validate(
      estimator=classifier,
      X=X,
      y=y,
      scoring=scoring,
      cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=rnd_seed + 6)
    )
    cv_scores = [cv_scores[f&#39;test_{scorer}&#39;].mean() for scorer in scoring]
    mean_cv_scores.append(cv_scores)
  return cv_scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again I will use the KMeans-SMOTE algorithm to rebalance the above dataset as well as the plain SMOTE oversampler for comparison. Using the above function we can calculate the out-of-sample performance of three different cases:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Calculate cross-validation scores
mapping = {&#39;No oversampling&#39;: None, &#39;SMOTE&#39;: SMOTE(), &#39;KMeans-SMOTE&#39;: SMOTE(clusterer=KMeans(n_clusters=60))}
cv_scores = {}
for name, oversampler in mapping.items():
  cv_scores[name] = calculate_cv_scores(oversampler, X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Printing a table of the scores, we see that KMeans-SMOTE outperforms the other methods when both geometric mean score and accuracy are used as evaluation metrics:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cv_scores = pd.DataFrame(cv_scores, index = [&#39;Geometric Mean&#39;, &#39;Accuracy&#39;])
print(cv_scores)

##########
# Output #
##########

#                 No oversampling     SMOTE  KMeans-SMOTE
# Geometric Mean         0.094281  0.364122      0.470664
# Accuracy               0.810000  0.780000      0.880000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that using the accuracy as an evaluation metric is not considered a good choice when the data is imbalanced. For example, a trivial classifier that always predicts the majority class would still have an accuracy equal to 0.90, even though all the minority class instances are misclassified. On the other hand, geometric mean score is an appropriate evaluation metric for imbalanced data since it equally weighs the accuracies per class.&lt;/p&gt;
&lt;p&gt;For more details you can look at the &lt;code&gt;cluster-over-sampling&lt;/code&gt; 
&lt;a href=&#34;https://cluster-over-sampling.readthedocs.io/en/latest/?badge=latest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;. The 
&lt;a href=&#34;https://imbalanced-learn.readthedocs.io/en/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; of the &lt;code&gt;imbalanced-learn&lt;/code&gt; project provides also various examples and an introduction to the imbalanced learning problem.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generative Oversampling</title>
      <link>https://aiandfriends.blog/site/project/generative-over-sampling/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/project/generative-over-sampling/</guid>
      <description>




  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://aiandfriends.blog/site/site/img/under_construction.png&#34; &gt;


  &lt;img src=&#34;https://aiandfriends.blog/site/site/img/under_construction.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Geometric SMOTE</title>
      <link>https://aiandfriends.blog/site/project/geometric-smote/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/project/geometric-smote/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Classification of imbalanced datasets is a challenging task for standard machine learning algorithms. Training a classifier on imbalanced data, often results in a low out-of-sample accuracy for the minority classes. To deal with this problem several approaches have been proposed. A general approach, known as oversampling, is the generation of artificial data for the minority classes that are used to enhance the training data.&lt;/p&gt;
&lt;p&gt;SMOTE is the most popular oversampling algorithm, while many variants of it have been developed. SMOTE generates synthetic data between the line segment that connects two randomly chosen neighboring minority class instances. On the other hand, Geometric SMOTE expands the data generation area by generating synthetic data inside a hypersphere that is defined by a randomly chosen minority class instance and one of its neighbors either from the minority or majority class. Geometric SMOTE 
&lt;a href=&#34;../../publication/gsmote_journal&#34;&gt;has been shown&lt;/a&gt; to outperform other standard oversamplers in a large number of datasets. The following figure illustrates the difference between the two data generation mechanisms:&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://aiandfriends.blog/site/site/project/geometric-smote/smote_vs_gsmote_huf5fcb04c55734e9a4ddd9006d03d5b15_12117_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;SMOTE vs Geometric SMOTE&#34;&gt;


  &lt;img data-src=&#34;https://aiandfriends.blog/site/site/project/geometric-smote/smote_vs_gsmote_huf5fcb04c55734e9a4ddd9006d03d5b15_12117_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;700px&#34; height=&#34;540&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    SMOTE vs Geometric SMOTE
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;A Python implementation of SMOTE and several of its variants is available in the 
&lt;a href=&#34;https://imbalanced-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imbalanced-Learn&lt;/a&gt; library, which is fully compatible with the popular machine learning toolbox 
&lt;a href=&#34;https://scikit-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scikit-Learn&lt;/a&gt;. I have developed a Python 
&lt;a href=&#34;https://github.com/AlgoWit/geometric-smote&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;implementation&lt;/a&gt; of Geometric SMOTE oversampler, called &lt;code&gt;geometric-smote&lt;/code&gt;, that integrates seamlessly with the Scikit-Learn and Imbalanced-Learn ecosystems.&lt;/p&gt;
&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;
&lt;p&gt;The easiest way to install the &lt;code&gt;geometric-smote&lt;/code&gt; package, assuming you have already Python 3 and &lt;code&gt;pip&lt;/code&gt; installed as well as you have optionally activated a Python virtual environment, is to open a shell and run the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install geometric-smote
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will install the latest version and its dependencies.&lt;/p&gt;
&lt;h2 id=&#34;documentation&#34;&gt;Documentation&lt;/h2&gt;
&lt;p&gt;Detailed documentation that includes installation guidelines, API description and various examples can found 
&lt;a href=&#34;https://geometric-smote.readthedocs.io/en/latest/?badge=latest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;functionality&#34;&gt;Functionality&lt;/h2&gt;
&lt;p&gt;In what follows, I will describe briefly some aspects of &lt;code&gt;geometric-smote&lt;/code&gt;&#39;s functionality. The class that represents the Geometric SMOTE oversampler is called &lt;code&gt;GeometricSMOTE&lt;/code&gt;. Its API follows closely the API of oversamplers provided by Imbalanced-Learn.&lt;/p&gt;
&lt;h4 id=&#34;resampling-an-imbalanced-dataset&#34;&gt;Resampling an imbalanced dataset&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s generate a binary class imbalanced dataset, represented by the input matrix &lt;code&gt;X&lt;/code&gt; and the target vector &lt;code&gt;y&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
from sklearn.datasets import make_classification

# Set random seed
rnd_seed = 43

# Generate imbalanced data
X, y = make_classification(
  n_samples=100,
  n_classes=2,
  weights=[0.9, 0.1],
  random_state=rnd_seed
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following functions extract and print the main characteristics of a binary class dataset. Specifically, the &lt;code&gt;extract_characteristics&lt;/code&gt; function returns the number of samples, the number of features,  the labels and the number of samples for the majority and minority classes as well as the Imbalance Ratio defined as the ratio between the number of samples of the majority and minority classes, while the &lt;code&gt;print_characteristics&lt;/code&gt; function prints them in an appropriate format:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
from collections import Counter

# Define function to extract dataset&#39;s characteristics
def extract_characteristics(X, y):
  n_samples, n_features = X.shape
  count_y = Counter(y)
  (maj_label, n_samples_maj), (min_label, n_samples_min) = count_y.most_common()
  ir = n_samples_maj / n_samples_min
  return n_samples, n_features, maj_label, n_samples_maj, min_label, n_samples_min, ir

# Define function to print dataset&#39;s characteristics
def print_characteristics(X, y):
  n_samples, n_features, maj_label, n_samples_maj, min_label, n_samples_min, ir = extract_characteristics(X, y)
  print(
    f&#39;Number of samples: {n_samples}&#39;,
    f&#39;Number of features: {n_features}&#39;,
    f&#39;Majority class label: {maj_label}&#39;,
    f&#39;Number of majority class samples: {n_samples_maj}&#39;,
    f&#39;Minority class label: {min_label}&#39;,
    f&#39;Number of minority class samples: {n_samples_min}&#39;,
    f&#39;Imbalance Ratio: {ir:.1f}&#39;,
    sep=&#39;\n&#39;
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I use the above function to print the main characteristics of the generated imbalanced dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print imbalanced dataset&#39;s characteristics
print_characteristics(X, y)

##########
# Output #
##########

# Number of samples: 100
# Number of features: 20
# Majority class label: 0
# Number of majority class samples: 90
# Minority class label: 1
# Number of minority class samples: 10
# Imbalance Ratio: 9.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Following the Imbalanced Learn&amp;rsquo;s API, the &lt;code&gt;fit_resample&lt;/code&gt; method of a &lt;code&gt;GeometricSMOTE&lt;/code&gt; instance can be used to resample the imbalanced dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
from gsmote import GeometricSMOTE

# Create GeometricSMOTE instance
geometric_smote = GeometricSMOTE(random_state=rnd_seed + 5)

# Fit and resample imbalanced data
X_res, y_res = geometric_smote.fit_resample(X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again we can print the main characteristics of the rebalanced dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Print balanced dataset&#39;s characteristics
print_characteristics(X_res, y_res)

##########
# Output #
##########

# Number of samples: 180
# Number of features: 20
# Majority class label: 0
# Number of majority class samples: 90
# Minority class label: 1
# Number of minority class samples: 90
# Imbalance Ratio: 1.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, the default behavior of the &lt;code&gt;GeometricSMOTE&lt;/code&gt; instance is to generate the appropriate number of minority class instances so that the resampled dataset is perfectly balanced.&lt;/p&gt;
&lt;h4 id=&#34;performance-on-out-of-sample-data&#34;&gt;Performance on out-of-sample data&lt;/h4&gt;
&lt;p&gt;As I mentioned above, training a classifier on imbalanced data may result in suboptimal performance on out-of-sample data. The function &lt;code&gt;calculate_cv_scores&lt;/code&gt; calculates the average 10-fold cross-validation geometric mean and accuracy scores across 100 runs of a decision tree classifier that is optionally combined to an oversampler through a pipeline:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_validate, StratifiedKFold
from sklearn.metrics import make_scorer, SCORERS
from imblearn.pipeline import make_pipeline
from imblearn.metrics import geometric_mean_score

# Append geometric mean score
SCORERS[&#39;g_mean&#39;] = make_scorer(geometric_mean_score)

# Define function that calculates out-of-sample scores
def calculate_cv_scores(oversampler, X, y):
  mean_cv_scores= []
  scoring = [&#39;g_mean&#39;, &#39;accuracy&#39;]
  n_runs = 100
  for ind in range(n_runs):
    rnd_seed = 10 * ind
    classifier = DecisionTreeClassifier(random_state=rnd_seed)
    if oversampler is not None:
      classifier = make_pipeline(
        oversampler.set_params(random_state=rnd_seed + 4), 
        classifier
      )
    cv_scores = cross_validate(
      estimator=classifier,
      X=X,
      y=y,
      scoring=scoring,
      cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=rnd_seed + 6)
    )
    cv_scores = [cv_scores[f&#39;test_{scorer}&#39;].mean() for scorer in scoring]
    mean_cv_scores.append(cv_scores)
  return cv_scores
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the above function we can calculate the out-of-sample performance when no oversampling is applied as well as when SMOTE and Geometric SMOTE are used as oversamplers:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
from imblearn.over_sampling import SMOTE

# Calculate cross-validation scores
mapping = {&#39;No oversampling&#39;: None, &#39;SMOTE&#39;: SMOTE(), &#39;Geometric SMOTE&#39;: GeometricSMOTE()}
cv_scores = {}
for name, oversampler in mapping.items():
  cv_scores[name] = calculate_cv_scores(oversampler, X, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Printing a table of the scores, we see that Geometric SMOTE outperforms the other methods when geometric mean score is used as an evaluation metric, while the highest accuracy is achieved when no oversampling is applied:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cv_scores = pd.DataFrame(cv_scores, index = [&#39;Geometric Mean&#39;, &#39;Accuracy&#39;])
print(cv_scores)

##########
# Output #
##########

#                 No oversampling     SMOTE  Geometric SMOTE
# Geometric Mean         0.782843  0.582843         0.841616
# Accuracy               0.950000  0.920000         0.870000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that using the accuracy as an evaluation metric is not considered a good choice when the data is imbalanced. For example, a trivial classifier that always predicts the majority class would still have an accuracy equal to 0.90, even though all the minority class instances are misclassified. On the other hand, geometric mean score is an appropriate evaluation metric for imbalanced data since it equally weighs the accuracies per class.&lt;/p&gt;
&lt;p&gt;For more details you can look at the &lt;code&gt;geometric-smote&lt;/code&gt; 
&lt;a href=&#34;https://geometric-smote.readthedocs.io/en/latest/?badge=latest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;. The 
&lt;a href=&#34;https://imbalanced-learn.readthedocs.io/en/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt; of the &lt;code&gt;imbalanced-learn&lt;/code&gt; project provides also various examples and an introduction to the imbalanced learning problem.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Research Learn</title>
      <link>https://aiandfriends.blog/site/project/research-learn/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/project/research-learn/</guid>
      <description>




  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://aiandfriends.blog/site/site/img/under_construction.png&#34; &gt;


  &lt;img src=&#34;https://aiandfriends.blog/site/site/img/under_construction.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>SOM for Scikit-Learn</title>
      <link>https://aiandfriends.blog/site/project/som-for-scikit-learn/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/project/som-for-scikit-learn/</guid>
      <description>




  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://aiandfriends.blog/site/site/img/under_construction.png&#34; &gt;


  &lt;img src=&#34;https://aiandfriends.blog/site/site/img/under_construction.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Sports Betting</title>
      <link>https://aiandfriends.blog/site/project/sports-betting/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/project/sports-betting/</guid>
      <description>




  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://aiandfriends.blog/site/site/img/under_construction.png&#34; &gt;


  &lt;img src=&#34;https://aiandfriends.blog/site/site/img/under_construction.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Το φαινόμενο Dunning-Krugger και ο Ηλίας Μάσκας</title>
      <link>https://aiandfriends.blog/site/post/dunning-krugger/</link>
      <pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/post/dunning-krugger/</guid>
      <description>&lt;h2 id=&#34;αυστηρός-ορισμός-του-προβλήματος&#34;&gt;Αυστηρός ορισμός του προβλήματος&lt;/h2&gt;
&lt;p&gt;Όλοι έχουμε βρεθεί σε μια παρέα στην οποία κάποιος φαίνεται να έχει απάντησεις για όλα τα θέματα. Έστω ότι τον λένε Μάκη. Ο Μάκης λοιπόν γνωρίζει απο διαστημική τεχνολογία, άρμεγμα της γίδας, διαφορική γεωμετρία σε πολλαπλότητες Calabi-Yau και οποιοδήποτε άλλο θέμα προκύψει πάνω στην κουβέντα. Όχι μόνο έχει απόψη αλλά μιλάει με την σιγουριά που θα εξηγούσε ένας νομπελίστας φυσικός κάποια θεωρία την οποία διατύπωσε ο ίδιος. Με ορατό τον κινδυνο να σε προσβάλλω και να σε χάσω απο πελάτη, θέλω να τονίσω ότι αν δεν αναγνωρίζεις κάποιον Μάκη στην παρέα σου είναι γιατί προφανώς ο Μάκης είσαι εσύ.&lt;/p&gt;
&lt;h2 id=&#34;η-επιστημονική-εξήγηση&#34;&gt;Η επιστημονική εξήγηση&lt;/h2&gt;
&lt;p&gt;Που θέλω να καταλήξω με όλα αυτά; Πολύ απλά και λαικά, το φαινόμενο του άχρηστου μπετόβλακα ο οποίος θεωρεί ότι ξέρει τα πάντα έχει ονομασία: Φαινόμενο Ντάνινγκ–Κρούγκερ. Απο την πάντα αξιόπιστη wikipedia:&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Το φαινόμενο Ντάνινγκ–Κρούγκερ είναι μια γνωστική διαταραχή στην οποία άτομα περιορισμένων δεξιοτήτων αποκτούν μια ψευδαίσθηση ανωτερώτητας, εκτιμώντας εσφαλμένα οτι οι γνωστικές τους ικανότητες, είναι υψηλότερες από ό,τι πραγματικά είναι.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Επειδή σε ότι κάνω χρησιμοποιώ αποκλειστικά την επιστημονική μέθοδο, το έψαξα αρκετά και τελικά κατάλαβα ότι ο ακαδημαικός Ντάνινγκ–Κρούγκερ είναι δύο διαφορετικοί άνθρωποι. Χωρίς να θέλω να κάνω τον έξυπνο, είναι πολυ λογικό γιατι υπάρχει η &amp;ldquo;-&amp;rdquo; ανάμεσα στα ονόματα. Έχεις δει ποτέ να γράφουμε Ζαν-Κλοντ-Βαν-Νταμ; Τέλος πάντων οι Ντάνινγκ–Κρούγκερ δεν μελέτησαν μόνο τους ηλίθιους αλλά και τους έξυπνους:&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Άτομα υψηλής ειδίκευσης μπορεί να υποτιμούν τη δική τους σχετική ικανότητά και μπορεί εσφαλμένα να υποθέτουν ότι τα καθήκοντα τα οποία είναι εύκολα για αυτούς είναι επίσης εύκολα για τους άλλους.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Το ζουμί της ιστορίας είναι το παρακάτω γράφημα:&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://aiandfriends.blog/site/site/post/dunning-krugger/simple-dn_hu0e5feadbcfd7edd8ed8410ba9b0c9b0a_85348_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Ωραίο γράφημα αλλά είναι παραμύθι.&#34;&gt;


  &lt;img data-src=&#34;https://aiandfriends.blog/site/site/post/dunning-krugger/simple-dn_hu0e5feadbcfd7edd8ed8410ba9b0c9b0a_85348_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;700px&#34; height=&#34;425&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Ωραίο γράφημα αλλά είναι παραμύθι.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Άρα βλέπουμε ότι αρχικά ο άχρηστος δεν ξέρει ούτε το όνομά του και προφανώς έχει μηδέν αυτοπεποίθηση. Με το που θα μάθει δύo πράγματα, η αυτοπεποίθησή του γρήγορα μέγιστοποιείται. Όταν λέω δύο πράγματα εννοώ να ανοίγει μια κονσέρβα ή να αλλάζει κανάλι. Αισθάνεται αυθεντία. Αν τελικά αποφασίσει να μάθει κάτι παραπάνω τότε καταλαβαίνει ότι δεν έχει ιδέα και δεν νιώθει σιγουριά για τις ικανότητές του. Στα δεξιά της καμπύλης ανήκουν οι πραγματικές αυθεντίες, όπως είναι ο Άδωνις Γεωργιάδης ή ο Νίκος Καρβέλας. Έχουν μπόλικη αυτοπεποίθηση αλλά σαφώς μικρότερη από αυτή που διαθέτει ο γκασμάς.&lt;/p&gt;
&lt;p&gt;Υποτίθεται ότι το γράφημα συνοψίζει με απλό τρόπο το φαινόμενο Ντάνινγκ–Κρούγκερ και αν το κοιτάξεις θα δείς ότι αναφέρεται σχεδόν σε όλα τα σχετικα άρθρα. Δυστυχώς ένα προβληματάκι
που έχει είναι ότι δεν βρίσκεται στις δημοσίευσεις των Ντάνινγκ–Κρούγκερ. Κάποιος το ζωγράφισε
στο Paint και οι υπόλοιποι το πίστεψαν. Ελπίζω δηλαδή, γιατί εγώ σίγουρα το πίστεψα. Εντάξει δεν πειράζει, ο σκοπός δεν είναι να δείξουμε τι ισχύει αλλά τι μας συμφέρει να ισχύει.
Να ένα πραγματικό γράφημα:&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://aiandfriends.blog/site/site/post/dunning-krugger/real-dn_huec9932930f769cd520f97ebec17b6af8_21871_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Αυτό είναι το πραγματικό ναούμε.&#34;&gt;


  &lt;img data-src=&#34;https://aiandfriends.blog/site/site/post/dunning-krugger/real-dn_huec9932930f769cd520f97ebec17b6af8_21871_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;700px&#34; height=&#34;246&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Αυτό είναι το πραγματικό ναούμε.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Το τελευταίο δείχνει ότι όσοι λένε κρυάδες θεωρούν ότι έχουν αρκετή πλάκα ενώ οι Σεφερλήδες υποτιμούν το πόσο αστείοι είναι. ΧΑΧΑΧΑΑΧΧΑΑΧ. Ξέσπασα.
Αντίστοιχα γραφήματα υπάρχουν και για άλλες δεξιότητες. Δηλαδή ο άχρηστος δεν θεωρεί ότι είναι αυθεντία αλλά ότι είναι καλύτερος απο το μέσο όρο.
Αυτό ακούγεται πιο λογικό σε σχέση με το προηγούμενο.&lt;/p&gt;
&lt;p&gt;Μεταγενέστερες μελέτες ασκούν κριτική στα ευρήματα των Ντάνινγκ–Κρούγκερ εντοπίζοντας μεθοδολογικά λάθη και υποστηρίζουν ότι όλοι υπερεκτιμούν τις ικανότητές τους.
Δεν παραθέτω τις πηγές μου, αν δεν σου αρέσει πήγαινε να διαβάσεις το Νature. Πάντως το τελευταίο συμπέρασμα με βολεύει και θα το κρατήσω για τη συνέχεια.&lt;/p&gt;
&lt;p&gt;Κλείνοντας θα ήθελα να σημειώσω ότι στους Ντάνινγκ–Κρούγκερ απονεμήθηκε το σατιρικό Ig Νόμπελ ψυχολογίας.
Δηλαδή οι Ντάνινγκ–Κρούγκερ μάλλον είχαν υπερεκτιμήσει την επιστημονική αξία του έργου τους. Φοβερό; Μόνος μου το σκέφτηκα.&lt;/p&gt;
&lt;h2 id=&#34;o-elon-musk&#34;&gt;O Elon Musk&lt;/h2&gt;
&lt;p&gt;Ομολογώ ότι δυσκολεύτηκα να αποφασίσω για το εαν θα έπρεπε να γράψω αυτά που ακολουθούν. Άρχικα με τον Elon μας συνδέει μια βαθιά φιλιά και εκτίμηση. Μπορεί να μην
με ξέρει αλλά η καταγωγή μας είναι κοινή. Όχι, ο Elon δεν είναι Πυργιώτης αλλά γεννήθηκε και μεγάλωσε στην Νότια Αφρική. Είναι γνωστό επίσης ότι και ο Homo
sapiens έζησε στην Αφρική πριν απο 200.000 χρόνια. Αυτό.&lt;/p&gt;
&lt;p&gt;Ο σημαντικότερος όμως ανασταλτικός παράγοντας για να ασκήσω κριτίκη στον Elon είναι οτι κάποια στιγμή στο μέλλον έχω δουλέψει σε μία απο τις εταιρίες του. Σε περίπτωση που η
προηγούμενη πρόταση δεν ακούγεται σωστή είναι γιατί αν θέλεις να συμβεί κάτι τότε το σύμπαν συνομώτει για να τα καταφέρεις. Αντίστροφα, αν δεν τα καταφέρεις είναι γιατί δεν συνομώτησε.
Τώρα αν ο Elon διαβάσει αυτό το άρθρο ενδέχεται να με αντιπαθήσει αλλά υποθέτοντας οτι τον πετυχαίνω στην ανάγκη, πιστεύω ότι θα αναγκαστεί να με προσλάβει στο μαγαζί του.
Το έχω δει να συμβαίνει live σε σουβλατζίδικο όταν ο ψήστης τσακώθηκε με το αφεντικό, παράτησε τα μπιφτέκια στη μέση αλλά μετά απο λίγο επέστρεψε για να συνεχίσει το ψήσιμο. Αν το καλοσκεφτείς και η Tesla ένα μεγάλο
σουβλατζίδικο είναι.&lt;/p&gt;
&lt;p&gt;Όσοι απο τους αναγνώστες μου έζησαν τα εφηβικά τους χρόνια στα 90s σίγουρα θα θυμούνται τον Τζίμη τον Αλήθεια. Μια απο τις πλέον χαρακτηριστικές ιστορίες του είναι αυτή με τη μεγάλη φωτιά στο χωριό του. Κάποια στιγμή το πυροσβεστικό αεροπλάνο επιστρέφει για ανεφοδιασμό. Τότε μαζί με το νερό, παίρνει και έναν καρχαρία ο οποίος έτυχε να κολυμπάει στην περιοχή υδροληψίας. Ρίχνοντας το νερό στο φλεγόμενο χωριό, &lt;strong&gt;ο καρχαρίας προσγειώνεται στο έδαφος και τρώει τα πρόβατα του παππού του&lt;/strong&gt;. Αν διαβάσει κανείς το Quora θα βρεί ιστορίες για τον Elon οι οποίες κάνουν το προηγούμενο σενάριο να μοιάζει σαν ένα αριστούργημα του σοσιαλιστικού ρεαλισμού. Μαθαίνουμε ότι υπήρξε παιδί θαυμα και μέχρι 2 ετών είχε διαβάσει ότι κυκλοφορούσε, δηλαδή απο τον κώδικα του Χαμουραμπί μέχρι κβαντική φυσική. Όταν τα υπόλοιπα παιδάκια έβλεπαν Thundercats αυτός διάβαζε Σιμόν ντε Μπoβουάρ. Αφου τα έμαθε όλα, άρχισε να εφαρμόζει τις γνώσεις του. Έφτιαξε απο μόνος του ένα video game, ένα μίξερ και ένα πολυόργανο Ηρακλής. Πήγε στο Stanford αλλά το παράτησε γιατί άκουσε το κάλεσμα απο τον ουρανό. Έπρεπε να σώσει την ανθρωπότητα. Δεν κοιμάται ποτέ, ίσως δεν τρώει. Ένας γραφικός Ινδός τον χαρακτήρισε ως metahuman. Τρέχει ταυτόχρονα 5000 projects και 10 εταιρείες. Φτιάχνει μπαταρίες, πυραύλους, είναι κοντά στην AI singularity και ετοιμάζεται να πεθάνει στον Άρη. Δεν έχει νόημα να συνεχίσω. Αυτός δεν είναι άνθρωπος. Το είπε εξάλλου ο Ινδός.&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://aiandfriends.blog/site/site/post/dunning-krugger/elon-musk_hudee8bd004f44aac87b0a70b4f971b441_403016_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Αυτός ο τύπος θα σώσει την ανθρωπότητα. Μια απλή καράφλα δεν είναι αρκετή για να τον σταματήσει.&#34;&gt;


  &lt;img data-src=&#34;https://aiandfriends.blog/site/site/post/dunning-krugger/elon-musk_hudee8bd004f44aac87b0a70b4f971b441_403016_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;700px&#34; height=&#34;403&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Αυτός ο τύπος θα σώσει την ανθρωπότητα. Μια απλή καράφλα δεν είναι αρκετή για να τον σταματήσει.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Όταν οι διάφοροι θαυμαστές του επαναλαμβάνουν συνεχώς τα παραπάνω με όλο και αυξανόμενη ένταση τότε είναι λογίκο να τα πιστέψει. Εδώ έχεις τη μάνα σου να λέει ότι είσαι το πιο όμορφο παιδάκι και τελικά το πιστεύεις, σκέψου να στο λένε άγνωστοι. Το αποτέλεσμα είναι αυτή η αηδία, η γελοιότητα, ο πιο εκνευριστικός κλαρινογαμπρός-εντερπρενέρ στο κόσμο, ο Elon Musk.&lt;/p&gt;
&lt;h2 id=&#34;μαθηματικό-μοντέλο&#34;&gt;Μαθηματικό μοντέλο&lt;/h2&gt;
&lt;p&gt;Ο Elon Musk λοιπόν είναι άλλος ένας άνθρωπος ο οποίος συστηματικά υπερεκτιμάει τις ικανότητές του. Προκλητικά αγνοεί τον παράγοντα τύχη και θεωρεί ότι είναι κλώνος του Νίκολα Τέσλα. Νομίζω ότι τώρα κάπως το έσωσα και το κούμπωσα με το φαινόμενο Ντάνινγκ–Κρούγκερ. Ευτυχώς γιατί είχα αρχίσει να ανησυχώ ότι θα χάσω το κοινό μου. Δεν είναι εκεί το θέμα όμως. Σκοπός μου είναι να δείξω πως με απλά μαθηματικά μπορεί κανείς να ποσοτικοποιήσει, σχεδόν να εξηγήσει, όλα τα παραπάνω.&lt;/p&gt;
&lt;p&gt;Πρίν μπώ δυνατά στις λεπτομέρειες και στο φορμαλισμό του μαθηματικού μοντέλου θα ήθελα να κάνω μια σύντομη εισαγωγή. Πιο συγκεκριμένα θα προσπαθήσω να περιγραψω πως οδηγήθηκα σε αυτό γιατί σύχνα εμείς οι επιστήμονες δίνουμε την εντύπωση ότι οι ιδέες μας γεννιούνται στο κενό. Ένα ερώτημα το οποίο με βασάνιζε απο 2.5 μηνών ήταν ποιό στοιχείο διαφοροποιεί τον άνθρωπο απο τα ζώα. Δεν ισχυρίζομαι ότι είμαι ο πρώτος που το σκέφτηκε. Ας είμαστε σοβαροί. Όμως είμαι ο πρώτος που το έλυσε και μάλιστα πριν απο 40 χρόνια:&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Η θεμελιώδης διαφορά ανάμεσα στον άνθρωπο και στα ζώα (λιοντάρια, κουνέλια, κόμπρες, άλογα κτλ) είναι η παπάτζα.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Έχεις δει ποτέ ζώο να πουλάει παραμύθι; Θα προσπαθήσει ποτέ η αρκούδα να σε πείσει ότι μπορεί να σε φάει; Η χελώνα θα σου εξηγήσει με επιχειρήματα ότι είναι πιο γρήγορη απο εσένα; Κατάλαβες που το πάω; Αν όχι, δεν πειράζει. Οι μεγάλες ιδέες δεν γίνονται άμεσα κατανοητές.&lt;/p&gt;





  
  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://aiandfriends.blog/site/site/post/dunning-krugger/priest_huef45f6c9876e80cf771040bddc4db553_208997_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Ο Elon Musk σε μια βραδινή έξοδο στο Lohan Nightclub.&#34;&gt;


  &lt;img data-src=&#34;https://aiandfriends.blog/site/site/post/dunning-krugger/priest_huef45f6c9876e80cf771040bddc4db553_208997_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;700px&#34; height=&#34;445&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Ο Elon Musk σε μια βραδινή έξοδο στο Lohan Nightclub.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Το μοντέλο μου λοιπόν ξεκινάει με την υπόθεση ότι η μόνη πραγματική δεξιότητα που υπάρχει στον άνθρωπο είναι η παπάτζα. Ομολογώ ότι είναι ισχυρή υπόθεση αλλά όπως είναι γνωστό:&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    All models are wrong but some are useful.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Το γράφω στα αγγλικά γιατί το ίδιο στα ελληνικά δεν γράφει ωραία. Δίχως να χάνω χρόνο, προχωρώ στο notation. Ονομάζω ως $PF$ (Priest Factor) τον παράγοντα ο οποίος μετράει την ικανότητα ενός ατόμου στη παπάτζα και θεωρώ ότι παίρνει τιμές στο σύνολο των πραγματικών αριθμών. Υποθέτω ότι η αυτοπεποίθηση ενός ατόμου, την οποία ονομάζω $CF$ (Confidence Factor), εξαρτάται απο την αλληλεπίδρασή του με κάποια άλλα άτομα. Αυτό είναι λογικό αν το σκεφτείς. Πάντα συγκρίνεις τον εαυτό σου με τους άλλους. Είσαι κοντός γιατί οι άλλοι είναι ψηλοί. Είσαι όμορφος γιατί οι άλλοι είναι μπάζα κ.ο.κ. Στη ζωή όλα είναι σχετικά σύμφωνα με τον Einstein (ή τη γυναίκα του αν πιστέψουμε τις διάφορες θωρίες συνομωσίας).&lt;/p&gt;
&lt;p&gt;Το κρίσιμο σημείο είναι ότι ο $CF$ ενός ατόμου είναι συνάρτηση του $PF$ του ίδιου ατόμου καθώς και των $PF_{i}$ των $i = 1, \cdots, n$ ατόμων με τα οποία αλληλεπιδρά. Ποιοτικά θα λέγαμε ότι αν βρέθεις σε μια παρέα στην οποία πουλάς τη μεγαλύτερη παπάτζα τότε όλοι σε πιστεύουν και η αυτοπεποίθησή σου πιάνει μέγιστο. Ομοίως αν είσαι ο ηλίθιος της παρέας, ή ισοδύναμα αυτός που έχει τη μικρότερη ικανότητα στο παραμύθι, τότε κανένας δεν σε πιστεύει και επειδή δεν το παίρνεις καν χαμπάρι (αφού οι υπόλοιποι σου πούλησαν το δικό τους ψέμα) πάλι έχεις μεγάλη σιγουριά για τον εαυτό σου. Το πρόβλημα εμφανίζεται όταν βρίσκονται στον ίδιο χώρο άτομα με αντίστοιχα επίπεδα παπάτζας. Τότε στην περίπτωση που είναι και οι δύο καλοί ψεύτες είναι αδύνατο να κοροιδέψει ο ένας τον άλλον. Αντίστοιχα εάν κανένας απο τους δύο δεν διαθέτει αυτή την ικανότητα τότε μένουν αποσβωλομένοι να κοιτούν με το χαρακτηριστικό βλέμα του πιγκουίνου περιμένοντας κάποιος να πεί το πρώτο ψέμα. Το αποτέλεσμα και στα δύο σενάρια είναι η χαμηλή αυτοπεποίθηση. Ποσοτικά η παραπάνω περιγραφή αντιστοιχεί στην εξής σχέση:&lt;/p&gt;
&lt;p&gt;$$CF = \frac{1}{n} \displaystyle\sum_{i=1}^n | PF - PF_{i} |$$&lt;/p&gt;
&lt;p&gt;δηλαδή η αυτοπεποίθηση ενός ατόμου είναι ίση με το άθροισμα των αποστάσεων ανάμεσα στη παπάτζα του και στην παπάτζα των υπολοίπων. Αυτό είναι πραγματικά deep result. Very deep.&lt;/p&gt;
&lt;p&gt;Υποθέτω δίχως δισταγμό ότι ο $PF_{i}$ για οποιοδήποτε απο αυτά τα ανεξάρτητα άτομα ακολουθεί μια τυποποιημένη κανονική κατανομή, δηλαδή:&lt;/p&gt;
&lt;p&gt;$$PF_{i} \stackrel{i.i.d.}{\sim} N(0, 1)$$&lt;/p&gt;
&lt;p&gt;Η ποσότητα που μας ενδιαφέρει είναι η αναμενόμενη τιμή του $CF$. Απο τις δύο προηγούμενες σχέσεις προκύπτει ότι ο $CF$ ακολουθεί τη λεγόμενη ως διπλωμένη κανονική κατανομή και συνεπώς:&lt;/p&gt;
&lt;p&gt;$E[CF] = \frac{2}{\pi} exp \Big( - \frac{PF}{2} \Big) - PF \cdot erf \Big( - \frac{PF}{\sqrt{2}} \Big)$&lt;/p&gt;
&lt;p&gt;όπου $erf$ η 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Error_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;συνάρτηση σφάλματος&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Θα σχεδιάσω την αναμενόμενη τιμή $E[CF]$ ως συνάρτηση του $PF$ για τιμές γύρω απο το μηδεν και τρείς τυπικές αποκλίσεις. Το παρακάτω απόσπασμα κώδικα σε Python 3 παράγει το ζητούμενο γράφημα:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Imports
from math import sqrt, exp, erf
import numpy as np
import matplotlib.pyplot as plt

# Define expectation of confidence factors
def calculate_confidence_factors(priest_factors):
    values = (2.0 / np.pi) * np.exp(-priest*factors / 2)
    corrections = priest_factors * np.erf(-priest_factors / np.sqrt(2))
    return values - corrections

# Generate priest and confidence factors values
priest_factors = np.linspace(-3.0, 3.0, 20)
expected_confidence_factors = calculate_confidence_factors(priest_factors)

# Plot
fig, ax = plt.subplots()
ax.plot(priest_factors, expected_confidence_factors)
ax.set(
    xlabel=&#39;Priest Factor&#39;,
    ylabel=&#39;Expected Confidence Factor&#39;,
    title=&#39;Dunning-Kruger Effect&#39;
)
ax.grid()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Η ομοιότητα με το υποτιθέμενο Ντάνινγκ–Κρούγκερ γράφημα είναι χωρίς υπερβολές ανατριχιαστική. Απο την συγκίνηση δυσκολέυομαι να γράψω ακόμα και τον επίλογο αυτού του άρθρου. Θα χαρακτήριζα την ανακάλυψη του μοντέλου ως μια μεταφυσική εμπειρία η οποία με άλλαξε ως άνθρωπο. Συγκεκριμένα μου προκάλεσε μια αστρική προβολή η οποία με ταξίδεψε στα όρια του γαλαξία μας. Ας αφήσω καλύτερα τις λεπτομέρειες για ένα επόμενο άρθρο.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Improving imbalanced learning through a heuristic oversampling method based on k-means and SMOTE</title>
      <link>https://aiandfriends.blog/site/publication/kmeans_smote_journal/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/publication/kmeans_smote_journal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Effective data generation for imbalanced learning using conditional generative adversarial networks</title>
      <link>https://aiandfriends.blog/site/publication/cgan/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/publication/cgan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Oversampling for Imbalanced Learning Based on K-Means and SMOTE</title>
      <link>https://aiandfriends.blog/site/publication/kmeans_smote_preprint/</link>
      <pubDate>Tue, 12 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/publication/kmeans_smote_preprint/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self-Organizing Map Oversampling (SOMO) for imbalanced data set learning</title>
      <link>https://aiandfriends.blog/site/publication/somo/</link>
      <pubDate>Sun, 01 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/publication/somo/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Geometric SMOTE: Effective oversampling for imbalanced learning through a geometric extension of SMOTE</title>
      <link>https://aiandfriends.blog/site/publication/gsmote_preprint/</link>
      <pubDate>Thu, 21 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/publication/gsmote_preprint/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Coset space dimensional reduction and Wilson flux breaking of ten-dimensional N=1, E(8) gauge theory</title>
      <link>https://aiandfriends.blog/site/publication/csdr2/</link>
      <pubDate>Fri, 12 Dec 2008 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/publication/csdr2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Coset space dimensional reduction and classification of semi‐realistic particle physics models</title>
      <link>https://aiandfriends.blog/site/publication/csdr/</link>
      <pubDate>Tue, 08 Apr 2008 00:00:00 +0000</pubDate>
      <guid>https://aiandfriends.blog/site/publication/csdr/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
