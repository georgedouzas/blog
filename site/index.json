[{"authors":["admin"],"categories":null,"content":"I work as a machine learning researcher at the Nova IMS, University of Lisbon. I am also a member of the MagIC research and development center. My research areas are physics, mathematics and artificial intelligence with multiple publications in machine learning and high energy physics journals. My professional experience includes working for various companies as a software and machine learning engineer. Additionally, I often maintain or contribute to open-source projects. Besides science, sports is my main hobby. Currently, my focus is on gymnastics while in the past I was a competitive sprinter. Lately, I have been working on various robotics projects.\n","date":1576540800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1576540800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://georgedouzas.github.io/site/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/site/authors/admin/","section":"authors","summary":"I work as a machine learning researcher at the Nova IMS, University of Lisbon. I am also a member of the MagIC research and development center. My research areas are physics, mathematics and artificial intelligence with multiple publications in machine learning and high energy physics journals. My professional experience includes working for various companies as a software and machine learning engineer. Additionally, I often maintain or contribute to open-source projects. Besides science, sports is my main hobby.","tags":null,"title":"Georgios Douzas","type":"authors"},{"authors":null,"categories":null,"content":"Program I train at a club following a non-competitive gymnastics program as a part of a group of athletes. I also work on my own on a variety of skills and strength exercises.\nGoals My long term gymnastics goals are the following:\n One arm chinup for 5 reps Full planche for 10 seconds Front lever for 10 seconds Hollow back press for 10 reps Manna for 10 seconds Handstand for 90 seconds One arm handstand for 10 seconds Handstand pushups for 10 reps L-sit press to handstand  ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1584316800,"objectID":"bf844f86df0364cb5fe58574fbed23e7","permalink":"https://georgedouzas.github.io/site/hobbies/gymnastics/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/site/hobbies/gymnastics/","section":"hobbies","summary":"Various videos showing my progression on gymnastics skills.","tags":null,"title":"Gymnastics","type":"docs"},{"authors":null,"categories":null,"content":"    ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"4ee04b944f5e0b86cc5aa8116203ae99","permalink":"https://georgedouzas.github.io/site/hobbies/robotics/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/site/hobbies/robotics/","section":"hobbies","summary":"Various robotics projects.","tags":null,"title":"Robotics","type":"docs"},{"authors":null,"categories":null,"content":"Pike press   L-sit to press handstand   Kneeling jump to handstand   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"99ed3aa781a68678dac47de21045aa65","permalink":"https://georgedouzas.github.io/site/hobbies/gymnastics/handstand/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/site/hobbies/gymnastics/handstand/","section":"hobbies","summary":"Pike press   L-sit to press handstand   Kneeling jump to handstand   ","tags":null,"title":"Handstand","type":"docs"},{"authors":null,"categories":null,"content":"Straddle planche   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"14d250e86c89d5aa85e5b3fbbda2f382","permalink":"https://georgedouzas.github.io/site/hobbies/gymnastics/planche/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/site/hobbies/gymnastics/planche/","section":"hobbies","summary":"Straddle planche   ","tags":null,"title":"Planche","type":"docs"},{"authors":["Georgios Douzas","Fernando Bacao","Joao Fonseca","Manvel Khudinyan"],"categories":null,"content":"","date":1576540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576540800,"objectID":"71b9d9670da92dd835084734bb9f7a8e","permalink":"https://georgedouzas.github.io/site/publication/land_cover_classification/","publishdate":"2019-12-17T00:00:00Z","relpermalink":"/site/publication/land_cover_classification/","section":"publication","summary":"The automatic production of land use/land cover maps continues to be a challenging problem, with important impacts on the ability to promote sustainability and good resource management. The ability to build robust automatic classifiers and produce accurate maps can have a significant impact on the way we manage and optimize natural resources. The difficulty in achieving these results comes from many different factors, such as data quality and uncertainty. In this paper, we address the imbalanced learning problem, a common and difficult conundrum in remote sensing that affects the quality of classification results, by proposing Geometric-SMOTE, a novel oversampling method, as a tool for addressing the imbalanced learning problem in remote sensing. Geometric-SMOTE is a sophisticated oversampling algorithm which increases the quality of the instances generated in previous methods, such as the synthetic minority oversampling technique. The performance of Geometric- SMOTE, in the LUCAS (Land Use/Cover Area Frame Survey) dataset, is compared to other oversamplers using a variety of classifiers. The results show that Geometric-SMOTE significantly outperforms all the other oversamplers and improves the robustness of the classifiers. These results indicate that, when using imbalanced datasets, remote sensing researchers should consider the use of these new generation oversamplers to increase the quality of the classification results.","tags":["Machine Learning","Remote Sensing","Imbalanced Learning Problem","Geometric SMOTE"],"title":"Imbalanced Learning in Land Cover Classification: Improving Minority Classes’ Prediction Accuracy Using the Geometric SMOTE Algorithm","type":"publication"},{"authors":["Georgios Douzas","Fernando Bacao"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"a66b9e7b77ae3899924c6953735b0aa0","permalink":"https://georgedouzas.github.io/site/publication/gsmote_journal/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/site/publication/gsmote_journal/","section":"publication","summary":"Classification of imbalanced datasets is a challenging task for standard algorithms. Although many methods exist to address this problem in different ways, generating artificial data for the minority class is a more general approach compared to algorithmic modifications. SMOTE algorithm, as well as any other oversampling method based on the SMOTE mechanism, generates synthetic samples along line segments that join minority class instances. In this paper we propose Geometric SMOTE (G-SMOTE) as a enhancement of the SMOTE data generation mechanism. G-SMOTE generates synthetic samples in a geometric region of the input space, around each selected minority instance. While in the basic configuration this region is a hyper-sphere, G-SMOTE allows its deformation to a hyper-spheroid. The performance of G-SMOTE is compared against SMOTE as well as baseline methods. We present empirical results that show a significant improvement in the quality of the generated data when G-SMOTE is used as an oversampling algorithm. An implementation of G-SMOTE is made available in the Python programming language.","tags":["Machine Learning","Imbalanced Learning Problem","Geometric SMOTE"],"title":"Geometric SMOTE a geometrically enhanced drop-in replacement for SMOTE","type":"publication"},{"authors":null,"categories":null,"content":"Introduction Learning from imbalanced data is a common and challenging problem in supervised learning. Training a classifier on imbalanced data, often results in a low out-of-sample accuracy for the minority classes. While different strategies exist to tackle this problem, the most general approach known as oversampling, is the generation of artificial data to achieve a balanced class distribution that in turn are used to enhance the training data.\nSMOTE algorithm, the most popular oversampler, as well as any other oversampling method based on it, generates synthetic samples along line segments that join minority class instances. SMOTE addresses only the issue of between-classes imbalance. On the other hand, by clustering the input space and applying any oversampling algorithm for each resulting cluster with appropriate resampling ratio, the within-classes imbalanced issue can be addressed. SOMO and KMeans-SMOTE are specific realizations of this approach that have been shown to outperform other standard oversamplers in a large number of datasets.\nA Python implementation of SMOTE and several of its variants is available in the Imbalanced-Learn library, which is fully compatible with the popular machine learning toolbox Scikit-Learn. I have developed a Python implementation of the above clustering-based oversampling approach, called cluster-over-sampling, that integrates seamlessly with the Scikit-Learn and Imbalanced-Learn ecosystems.\nInstallation The easiest way to install the cluster-over-sampling package, assuming you have already Python 3 and pip installed as well as you have optionally activated a Python virtual environment, is to open a shell and run the following command:\npip install cluster-over-sampling  This will install the latest version and its dependencies.\nDocumentation Detailed documentation that includes installation guidelines, API description and various examples can found here.\nFunctionality In what follows, I will describe briefly some aspects of cluster-over-sampling's functionality. The class that represents the Geometric SMOTE oversampler is called GeometricSMOTE. Its API follows closely the API of oversamplers provided by Imbalanced-Learn.\nResampling an imbalanced dataset Let\u0026rsquo;s generate a binary class imbalanced dataset, represented by the input matrix X and the target vector y:\n# Imports from sklearn.datasets import make_classification # Set random seed rnd_seed = 43 # Generate imbalanced data X, y = make_classification( n_samples=100, n_classes=2, weights=[0.9, 0.1], random_state=rnd_seed )  The following functions extract and print the main characteristics of a binary class dataset. Specifically, the extract_characteristics function returns the number of samples, the number of features, the labels and the number of samples for the majority and minority classes as well as the Imbalance Ratio defined as the ratio between the number of samples of the majority and minority classes, while the print_characteristics funcion prints them in an appropriate format:\n# Imports from collections import Counter # Define function to extract dataset's characteristics def extract_characteristics(X, y): n_samples, n_features = X.shape count_y = Counter(y) (maj_label, n_samples_maj), (min_label, n_samples_min) = count_y.most_common() ir = n_samples_maj / n_samples_min return n_samples, n_features, maj_label, n_samples_maj, min_label, n_samples_min, ir # Define function to print dataset's characteristics def print_characteristics(X, y): n_samples, n_features, maj_label, n_samples_maj, min_label, n_samples_min, ir = extract_characteristics(X, y) print( f'Number of samples: {n_samples}', f'Number of features: {n_features}', f'Majority class label: {maj_label}', f'Number of majority class samples: {n_samples_maj}', f'Minority class label: {min_label}', f'Number of minority class samples: {n_samples_min}', f'Imbalance Ratio: {ir:.1f}', sep='\\n' )  I use the above function to print the main characteristics of the generated imbalanced dataset:\n# Print imbalanced dataset's characteristics print_characteristics(X, y) ########## # Output # ########## # Number of samples: 100 # Number of features: 20 # Majority class label: 0 # Number of majority class samples: 90 # Minority class label: 1 # Number of minority class samples: 10 # Imbalance Ratio: 9.0  Following the Imbalanced Learn\u0026rsquo;s API, the fit_resample method of a GeometricSMOTE instance can be used to resample the imbalanced dataset:\n# Imports from gsmote import GeometricSMOTE # Create GeometricSMOTE instance geometric_smote = GeometricSMOTE(random_state=rnd_seed + 5) # Fit and resample imbalanced data X_res, y_res = geometric_smote.fit_resample(X, y)  Again we can print the main characteristics of the rebalanced dataset:\n# Print balanced dataset's characteristics print_characteristics(X_res, y_res) ########## # Output # ########## # Number of samples: 180 # Number of features: 20 # Majority class label: 0 # Number of majority class samples: 90 # Minority class label: 1 # Number of minority class samples: 90 # Imbalance Ratio: 1.0  As expected, the default behaviour of the GeometricSMOTE instance is to generate the apropriate number of minority class instances so that the resampled dataset is perfeclty balanced.\nPerformance on out-of-sample data As I mentioned above, training a classifier on imbalanced data may result in suboptimal performance on out-of-sample data. The function calculate_cv_scores calculates the average 10-fold cross-validation geometric mean and accuracy scores across 100 runs of a decision tree classifier:\n# Imports import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_validate, StratifiedKFold from sklearn.metrics import make_scorer, SCORERS from imblearn.pipeline import make_pipeline from imblearn.metrics import geometric_mean_score # Append geometric mean score SCORERS['g_mean'] = make_scorer(geometric_mean_score) # Define function that calculates out-of-sample scores def calculate_cv_scores(oversampler, X, y): mean_cv_scores= [] scoring = ['g_mean', 'accuracy'] n_runs = 100 for ind in range(n_runs): rnd_seed = 10 * ind classifier = DecisionTreeClassifier(random_state=rnd_seed) if oversampler is not None: classifier = make_pipeline( oversampler.set_params(random_state=rnd_seed + 4), classifier ) cv_scores = cross_validate( estimator=classifier, X=X, y=y, scoring=scoring, cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=rnd_seed + 6) ) cv_scores = [cv_scores[f'test_{scorer}'].mean() for scorer in scoring] mean_cv_scores.append(cv_scores) return cv_scores  Using the above function we can calculate the out-of-sample performance when no oversampling is applied as well as when SMOTE and Geometric SMOTE are used as oversamplers:\n# Imports from imblearn.over_sampling import SMOTE # Calculate cross-validation scores mapping = {'No oversampling': None, 'SMOTE': SMOTE(), 'Geometric SMOTE': GeometricSMOTE()} cv_scores = {} for name, oversampler in mapping.items(): cv_scores[name] = calculate_cv_score(oversampler, X, y)  Printing a table of the scores, we see that Geometric SMOTE outperforms the other methods when geometric mean score is used as an evaluation metric, while the highest accuracy is achieved when no oversampling is applied:\ncv_scores = pd.DataFrame(cv_scores, index = ['Geometric Mean', 'Accuracy']) print(cv_scores) ########## # Output # ########## # No oversampling SMOTE Geometric SMOTE # Geometric Mean 0.782843 0.582843 0.841616 # Accuracy 0.950000 0.920000 0.870000  Notice that using the accuracy as an evaluation metric is not considered a good choice when the data is imbalanced. For example, a trivial classifier that always predicts the majority class would still have an accuracy equal to 0.90, even though all the minority class instances are misclassified. On the other hand, geometric mean score is an appropriate evaluation metric for imbalanced data since it equaly weighs the accuracies per class.\nFor more details you can look at the geometric-smote documentation. The documentation of the imbalanced-learn project provides also various examples and an introduction to the imbalanced learning problem.\n","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"89149b80b88aaf8ec3e87f1aeea58a66","permalink":"https://georgedouzas.github.io/site/project/clustering-based-oversampling/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/site/project/clustering-based-oversampling/","section":"project","summary":"A Python package for combining clustering and oversampling.","tags":["Machine Learning","Imbalanced Learning Problem","Clustering","Production"],"title":"Clustering-Based Oversampling","type":"project"},{"authors":null,"categories":null,"content":"    ","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"cc03fbd2aeec70902d627080e9ef85a7","permalink":"https://georgedouzas.github.io/site/project/generative-over-sampling/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/site/project/generative-over-sampling/","section":"project","summary":"A Scikit-Learn compatible framework that supports generative models as oversamplers.","tags":["Machine Learning","Deep Learning","Generative Models","Production"],"title":"Generative Oversampling","type":"project"},{"authors":null,"categories":null,"content":"Introduction Classification of imbalanced datasets is a challenging task for standard machine learning algorithms. Training a classifier on imbalanced data, often results in a low out-of-sample accuracy for the minority classes. To deal with this problem several approaches have been proposed. A general approach, known as oversampling, is the generation of artificial data for the minority classes that are used to enhance the training data.\nSMOTE is the most popular oversampling algorithm, while many variants of it have been developed. SMOTE generates synthetic data between the line segment that connects two randomly chosen neighboring minority class instances. On the other hand, Geometric SMOTE epands the data generation area by generating synthetic data inside a hypersphere that is defined by a randomly chosen minority class instance and one of its neighbors either from the minority or majority class. Geometric SMOTE has been shown to outperform other standard oversamplers in a large number of datasets. The following figure illustrates the difference between the two data generation mechanisms:\n   SMOTE vs Geometric SMOTE   A Python implementation of SMOTE and several of its variants is available in the Imbalanced-Learn library, which is fully compatible with the popular machine learning toolbox Scikit-Learn. I have developed a Python implementation of Geometric SMOTE oversampler, called geometric-smote, that integrates seamlessly with the Scikit-Learn and Imbalanced-Learn ecosystems.\nInstallation The easiest way to install the geometric-smote package, assuming you have already Python 3 and pip installed as well as you have optionally activated a Python virtual environment, is to open a shell and run the following command:\npip install geometric-smote  This will install the latest version and its dependencies.\nDocumentation Detailed documentation that includes installation guidelines, API description and various examples can found here.\nFunctionality In what follows, I will describe briefly some aspects of geometric-smote's functionality. The class that represents the Geometric SMOTE oversampler is called GeometricSMOTE. Its API follows closely the API of oversamplers provided by Imbalanced-Learn.\nResampling an imbalanced dataset Let\u0026rsquo;s generate a binary class imbalanced dataset, represented by the input matrix X and the target vector y:\n# Imports from sklearn.datasets import make_classification # Set random seed rnd_seed = 43 # Generate imbalanced data X, y = make_classification( n_samples=100, n_classes=2, weights=[0.9, 0.1], random_state=rnd_seed )  The following functions extract and print the main characteristics of a binary class dataset. Specifically, the extract_characteristics function returns the number of samples, the number of features, the labels and the number of samples for the majority and minority classes as well as the Imbalance Ratio defined as the ratio between the number of samples of the majority and minority classes, while the print_characteristics funcion prints them in an appropriate format:\n# Imports from collections import Counter # Define function to extract dataset's characteristics def extract_characteristics(X, y): n_samples, n_features = X.shape count_y = Counter(y) (maj_label, n_samples_maj), (min_label, n_samples_min) = count_y.most_common() ir = n_samples_maj / n_samples_min return n_samples, n_features, maj_label, n_samples_maj, min_label, n_samples_min, ir # Define function to print dataset's characteristics def print_characteristics(X, y): n_samples, n_features, maj_label, n_samples_maj, min_label, n_samples_min, ir = extract_characteristics(X, y) print( f'Number of samples: {n_samples}', f'Number of features: {n_features}', f'Majority class label: {maj_label}', f'Number of majority class samples: {n_samples_maj}', f'Minority class label: {min_label}', f'Number of minority class samples: {n_samples_min}', f'Imbalance Ratio: {ir:.1f}', sep='\\n' )  I use the above function to print the main characteristics of the generated imbalanced dataset:\n# Print imbalanced dataset's characteristics print_characteristics(X, y) ########## # Output # ########## # Number of samples: 100 # Number of features: 20 # Majority class label: 0 # Number of majority class samples: 90 # Minority class label: 1 # Number of minority class samples: 10 # Imbalance Ratio: 9.0  Following the Imbalanced Learn\u0026rsquo;s API, the fit_resample method of a GeometricSMOTE instance can be used to resample the imbalanced dataset:\n# Imports from gsmote import GeometricSMOTE # Create GeometricSMOTE instance geometric_smote = GeometricSMOTE(random_state=rnd_seed + 5) # Fit and resample imbalanced data X_res, y_res = geometric_smote.fit_resample(X, y)  Again we can print the main characteristics of the rebalanced dataset:\n# Print balanced dataset's characteristics print_characteristics(X_res, y_res) ########## # Output # ########## # Number of samples: 180 # Number of features: 20 # Majority class label: 0 # Number of majority class samples: 90 # Minority class label: 1 # Number of minority class samples: 90 # Imbalance Ratio: 1.0  As expected, the default behaviour of the GeometricSMOTE instance is to generate the apropriate number of minority class instances so that the resampled dataset is perfeclty balanced.\nPerformance on out-of-sample data As I mentioned above, training a classifier on imbalanced data may result in suboptimal performance on out-of-sample data. The function calculate_cv_scores calculates the average 10-fold cross-validation geometric mean and accuracy scores across 100 runs of a decision tree classifier:\n# Imports import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_validate, StratifiedKFold from sklearn.metrics import make_scorer, SCORERS from imblearn.pipeline import make_pipeline from imblearn.metrics import geometric_mean_score # Append geometric mean score SCORERS['g_mean'] = make_scorer(geometric_mean_score) # Define function that calculates out-of-sample scores def calculate_cv_scores(oversampler, X, y): mean_cv_scores= [] scoring = ['g_mean', 'accuracy'] n_runs = 100 for ind in range(n_runs): rnd_seed = 10 * ind classifier = DecisionTreeClassifier(random_state=rnd_seed) if oversampler is not None: classifier = make_pipeline( oversampler.set_params(random_state=rnd_seed + 4), classifier ) cv_scores = cross_validate( estimator=classifier, X=X, y=y, scoring=scoring, cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=rnd_seed + 6) ) cv_scores = [cv_scores[f'test_{scorer}'].mean() for scorer in scoring] mean_cv_scores.append(cv_scores) return cv_scores  Using the above function we can calculate the out-of-sample performance when no oversampling is applied as well as when SMOTE and Geometric SMOTE are used as oversamplers:\n# Imports from imblearn.over_sampling import SMOTE # Calculate cross-validation scores mapping = {'No oversampling': None, 'SMOTE': SMOTE(), 'Geometric SMOTE': GeometricSMOTE()} cv_scores = {} for name, oversampler in mapping.items(): cv_scores[name] = calculate_cv_score(oversampler, X, y)  Printing a table of the scores, we see that Geometric SMOTE outperforms the other methods when geometric mean score is used as an evaluation metric, while the highest accuracy is achieved when no oversampling is applied:\ncv_scores = pd.DataFrame(cv_scores, index = ['Geometric Mean', 'Accuracy']) print(cv_scores) ########## # Output # ########## # No oversampling SMOTE Geometric SMOTE # Geometric Mean 0.782843 0.582843 0.841616 # Accuracy 0.950000 0.920000 0.870000  Notice that using the accuracy as an evaluation metric is not considered a good choice when the data is imbalanced. For example, a trivial classifier that always predicts the majority class would still have an accuracy equal to 0.90, even though all the minority class instances are misclassified. On the other hand, geometric mean score is an appropriate evaluation metric for imbalanced data since it equaly weighs the accuracies per class.\nFor more details you can look at the geometric-smote documentation. The documentation of the imbalanced-learn project provides also various examples and an introduction to the imbalanced learning problem.\n","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"f2a6d86412a52d5c91d923c21218e26f","permalink":"https://georgedouzas.github.io/site/project/geometric-smote/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/site/project/geometric-smote/","section":"project","summary":"A Python package for flexible and efficient oversampling.","tags":["Machine Learning","Imbalanced Learning Problem","Geometric SMOTE","Production"],"title":"Geometric SMOTE","type":"project"},{"authors":null,"categories":null,"content":"    ","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"ea204258f206bdf86d31309f1c33a607","permalink":"https://georgedouzas.github.io/site/project/research-learn/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/site/project/research-learn/","section":"project","summary":"Toolbox for reproducible research in machine learning.","tags":["Machine Learning","Research","Production"],"title":"Research Learn","type":"project"},{"authors":null,"categories":null,"content":"    ","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"eaa71e8c89a168393b83c71eb800eb82","permalink":"https://georgedouzas.github.io/site/project/som-for-scikit-learn/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/site/project/som-for-scikit-learn/","section":"project","summary":"A Scikit-Learn compatible implementation of Self-Organizing Map based on Somoclu.","tags":["Machine Learning","Clustering","Production"],"title":"SOM for Scikit-Learn","type":"project"},{"authors":null,"categories":null,"content":"    ","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"f18ed7de07b8155148b314e0d041264f","permalink":"https://georgedouzas.github.io/site/project/sports-betting/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/site/project/sports-betting/","section":"project","summary":"Develop machine learning models to beat the bookmakers.","tags":["Machine Learning","Sports Betting","Production"],"title":"Sports Betting","type":"project"},{"authors":null,"categories":null,"content":"Αυστηρός ορισμός του προβλήματος Όλοι έχουμε βρεθεί σε μια παρέα στην οποία κάποιος φαίνεται να έχει απάντησεις για όλα τα θέματα. Έστω ότι τον λένε Μάκη. Ο Μάκης λοιπόν γνωρίζει απο διαστημική τεχνολογία, άρμεγμα της γίδας, διαφορική γεωμετρία σε πολλαπλότητες Calabi-Yau και οποιοδήποτε άλλο θέμα προκύψει πάνω στην κουβέντα. Όχι μόνο έχει απόψη αλλά μιλάει με την σιγουριά που θα εξηγούσε ένας νομπελίστας φυσικός κάποια θεωρία την οποία διατύπωσε ο ίδιος. Με ορατό τον κινδυνο να σε προσβάλλω και να σε χάσω απο πελάτη, θέλω να τονίσω ότι αν δεν αναγνωρίζεις κάποιον Μάκη στην παρέα σου είναι γιατί προφανώς ο Μάκης είσαι εσύ.\nΗ επιστημονική εξήγηση Που θέλω να καταλήξω με όλα αυτά; Πολύ απλά και λαικά, το φαινόμενο του άχρηστου μπετόβλακα ο οποίος θεωρεί ότι ξέρει τα πάντα έχει ονομασία: Φαινόμενο Ντάνινγκ–Κρούγκερ. Απο την πάντα αξιόπιστη wikipedia:\n Το φαινόμενο Ντάνινγκ–Κρούγκερ είναι μια γνωστική διαταραχή στην οποία άτομα περιορισμένων δεξιοτήτων αποκτούν μια ψευδαίσθηση ανωτερώτητας, εκτιμώντας εσφαλμένα οτι οι γνωστικές τους ικανότητες, είναι υψηλότερες από ό,τι πραγματικά είναι.   Επειδή σε ότι κάνω χρησιμοποιώ αποκλειστικά την επιστημονική μέθοδο, το έψαξα αρκετά και τελικά κατάλαβα ότι ο ακαδημαικός Ντάνινγκ–Κρούγκερ είναι δύο διαφορετικοί άνθρωποι. Χωρίς να θέλω να κάνω τον έξυπνο, είναι πολυ λογικό γιατι υπάρχει η \u0026ldquo;-\u0026rdquo; ανάμεσα στα ονόματα. Έχεις δει ποτέ να γράφουμε Ζαν-Κλοντ-Βαν-Νταμ; Τέλος πάντων οι Ντάνινγκ–Κρούγκερ δεν μελέτησαν μόνο τους ηλίθιους αλλά και τους έξυπνους:\n Άτομα υψηλής ειδίκευσης μπορεί να υποτιμούν τη δική τους σχετική ικανότητά και μπορεί εσφαλμένα να υποθέτουν ότι τα καθήκοντα τα οποία είναι εύκολα για αυτούς είναι επίσης εύκολα για τους άλλους.   Το ζουμί της ιστορίας είναι το παρακάτω γράφημα:\n   Ωραίο γράφημα αλλά είναι παραμύθι.   Άρα βλέπουμε ότι αρχικά ο άχρηστος δεν ξέρει ούτε το όνομά του και προφανώς έχει μηδέν αυτοπεποίθηση. Με το που θα μάθει δύo πράγματα, η αυτοπεποίθησή του γρήγορα μέγιστοποιείται. Όταν λέω δύο πράγματα εννοώ να ανοίγει μια κονσέρβα ή να αλλάζει κανάλι. Αισθάνεται αυθεντία. Αν τελικά αποφασίσει να μάθει κάτι παραπάνω τότε καταλαβαίνει ότι δεν έχει ιδέα και δεν νιώθει σιγουριά για τις ικανότητές του. Στα δεξιά της καμπύλης ανήκουν οι πραγματικές αυθεντίες, όπως είναι ο Άδωνις Γεωργιάδης ή ο Νίκος Καρβέλας. Έχουν μπόλικη αυτοπεποίθηση αλλά σαφώς μικρότερη από αυτή που διαθέτει ο γκασμάς.\nΥποτίθεται ότι το γράφημα συνοψίζει με απλό τρόπο το φαινόμενο Ντάνινγκ–Κρούγκερ και αν το κοιτάξεις θα δείς ότι αναφέρεται σχεδόν σε όλα τα σχετικα άρθρα. Δυστυχώς ένα προβληματάκι που έχει είναι ότι δεν βρίσκεται στις δημοσίευσεις των Ντάνινγκ–Κρούγκερ. Κάποιος το ζωγράφισε στο Paint και οι υπόλοιποι το πίστεψαν. Ελπίζω δηλαδή, γιατί εγώ σίγουρα το πίστεψα. Εντάξει δεν πειράζει, ο σκοπός δεν είναι να δείξουμε τι ισχύει αλλά τι μας συμφέρει να ισχύει. Να ένα πραγματικό γράφημα:\n   Αυτό είναι το πραγματικό ναούμε.   Το τελευταίο δείχνει ότι όσοι λένε κρυάδες θεωρούν ότι έχουν αρκετή πλάκα ενώ οι Σεφερλήδες υποτιμούν το πόσο αστείοι είναι. ΧΑΧΑΧΑΑΧΧΑΑΧ. Ξέσπασα. Αντίστοιχα γραφήματα υπάρχουν και για άλλες δεξιότητες. Δηλαδή ο άχρηστος δεν θεωρεί ότι είναι αυθεντία αλλά ότι είναι καλύτερος απο το μέσο όρο. Αυτό ακούγεται πιο λογικό σε σχέση με το προηγούμενο.\nΜεταγενέστερες μελέτες ασκούν κριτική στα ευρήματα των Ντάνινγκ–Κρούγκερ εντοπίζοντας μεθοδολογικά λάθη και υποστηρίζουν ότι όλοι υπερεκτιμούν τις ικανότητές τους. Δεν παραθέτω τις πηγές μου, αν δεν σου αρέσει πήγαινε να διαβάσεις το Νature. Πάντως το τελευταίο συμπέρασμα με βολεύει και θα το κρατήσω για τη συνέχεια.\nΚλείνοντας θα ήθελα να σημειώσω ότι στους Ντάνινγκ–Κρούγκερ απονεμήθηκε το σατιρικό Ig Νόμπελ ψυχολογίας. Δηλαδή οι Ντάνινγκ–Κρούγκερ μάλλον είχαν υπερεκτιμήσει την επιστημονική αξία του έργου τους. Φοβερό; Μόνος μου το σκέφτηκα.\nO Elon Musk Ομολογώ ότι δυσκολεύτηκα να αποφασίσω για το εαν θα έπρεπε να γράψω αυτά που ακολουθούν. Άρχικα με τον Elon μας συνδέει μια βαθιά φιλιά και εκτίμηση. Μπορεί να μην με ξέρει αλλά η καταγωγή μας είναι κοινή. Όχι, ο Elon δεν είναι Πυργιώτης αλλά γεννήθηκε και μεγάλωσε στην Νότια Αφρική. Είναι γνωστό επίσης ότι και ο Homo sapiens έζησε στην Αφρική πριν απο 200.000 χρόνια. Αυτό.\nΟ σημαντικότερος όμως ανασταλτικός παράγοντας για να ασκήσω κριτίκη στον Elon είναι οτι κάποια στιγμή στο μέλλον έχω δουλέψει σε μία απο τις εταιρίες του. Σε περίπτωση που η προηγούμενη πρόταση δεν ακούγεται σωστή είναι γιατί αν θέλεις να συμβεί κάτι τότε το σύμπαν συνομώτει για να τα καταφέρεις. Αντίστροφα, αν δεν τα καταφέρεις είναι γιατί δεν συνομώτησε. Τώρα αν ο Elon διαβάσει αυτό το άρθρο ενδέχεται να με αντιπαθήσει αλλά υποθέτοντας οτι τον πετυχαίνω στην ανάγκη, πιστεύω ότι θα αναγκαστεί να με προσλάβει στο μαγαζί του. Το έχω δει να συμβαίνει live σε σουβλατζίδικο όταν ο ψήστης τσακώθηκε με το αφεντικό, παράτησε τα μπιφτέκια στη μέση αλλά μετά απο λίγο επέστρεψε για να συνεχίσει το ψήσιμο. Αν το καλοσκεφτείς και η Tesla ένα μεγάλο σουβλατζίδικο είναι.\nΌσοι απο τους αναγνώστες μου έζησαν τα εφηβικά τους χρόνια στα 90s σίγουρα θα θυμούνται τον Τζίμη τον Αλήθεια. Μια απο τις πλέον χαρακτηριστικές ιστορίες του είναι αυτή με τη μεγάλη φωτιά στο χωριό του. Κάποια στιγμή το πυροσβεστικό αεροπλάνο επιστρέφει για ανεφοδιασμό. Τότε μαζί με το νερό, παίρνει και έναν καρχαρία ο οποίος έτυχε να κολυμπάει στην περιοχή υδροληψίας. Ρίχνοντας το νερό στο φλεγόμενο χωριό, ο καρχαρίας προσγειώνεται στο έδαφος και τρώει τα πρόβατα του παππού του. Αν διαβάσει κανείς το Quora θα βρεί ιστορίες για τον Elon οι οποίες κάνουν το προηγούμενο σενάριο να μοιάζει σαν ένα αριστούργημα του σοσιαλιστικού ρεαλισμού. Μαθαίνουμε ότι υπήρξε παιδί θαυμα και μέχρι 2 ετών είχε διαβάσει ότι κυκλοφορούσε, δηλαδή απο τον κώδικα του Χαμουραμπί μέχρι κβαντική φυσική. Όταν τα υπόλοιπα παιδάκια έβλεπαν Thundercats αυτός διάβαζε Σιμόν ντε Μπoβουάρ. Αφου τα έμαθε όλα, άρχισε να εφαρμόζει τις γνώσεις του. Έφτιαξε απο μόνος του ένα video game, ένα μίξερ και ένα πολυόργανο Ηρακλής. Πήγε στο Stanford αλλά το παράτησε γιατί άκουσε το κάλεσμα απο τον ουρανό. Έπρεπε να σώσει την ανθρωπότητα. Δεν κοιμάται ποτέ, ίσως δεν τρώει. Ένας γραφικός Ινδός τον χαρακτήρισε ως metahuman. Τρέχει ταυτόχρονα 5000 projects και 10 εταιρείες. Φτιάχνει μπαταρίες, πυραύλους, είναι κοντά στην AI singularity και ετοιμάζεται να πεθάνει στον Άρη. Δεν έχει νόημα να συνεχίσω. Αυτός δεν είναι άνθρωπος. Το είπε εξάλλου ο Ινδός.\n   Αυτός ο τύπος θα σώσει την ανθρωπότητα. Μια απλή καράφλα δεν είναι αρκετή για να τον σταματήσει.   Όταν οι διάφοροι θαυμαστές του επαναλαμβάνουν συνεχώς τα παραπάνω με όλο και αυξανόμενη ένταση τότε είναι λογίκο να τα πιστέψει. Εδώ έχεις τη μάνα σου να λέει ότι είσαι το πιο όμορφο παιδάκι και τελικά το πιστεύεις, σκέψου να στο λένε άγνωστοι. Το αποτέλεσμα είναι αυτή η αηδία, η γελοιότητα, ο πιο εκνευριστικός κλαρινογαμπρός-εντερπρενέρ στο κόσμο, ο Elon Musk.\nΜαθηματικό μοντέλο Ο Elon Musk λοιπόν είναι άλλος ένας άνθρωπος ο οποίος συστηματικά υπερεκτιμάει τις ικανότητές του. Προκλητικά αγνοεί τον παράγοντα τύχη και θεωρεί ότι είναι κλώνος του Νίκολα Τέσλα. Νομίζω ότι τώρα κάπως το έσωσα και το κούμπωσα με το φαινόμενο Ντάνινγκ–Κρούγκερ. Ευτυχώς γιατί είχα αρχίσει να ανησυχώ ότι θα χάσω το κοινό μου. Δεν είναι εκεί το θέμα όμως. Σκοπός μου είναι να δείξω πως με απλά μαθηματικά μπορεί κανείς να ποσοτικοποιήσει, σχεδόν να εξηγήσει, όλα τα παραπάνω.\nΠρίν μπώ δυνατά στις λεπτομέρειες και στο φορμαλισμό του μαθηματικού μοντέλου θα ήθελα να κάνω μια σύντομη εισαγωγή. Πιο συγκεκριμένα θα προσπαθήσω να περιγραψω πως οδηγήθηκα σε αυτό γιατί σύχνα εμείς οι επιστήμονες δίνουμε την εντύπωση ότι οι ιδέες μας γεννιούνται στο κενό. Ένα ερώτημα το οποίο με βασάνιζε απο 2.5 μηνών ήταν ποιό στοιχείο διαφοροποιεί τον άνθρωπο απο τα ζώα. Δεν ισχυρίζομαι ότι είμαι ο πρώτος που το σκέφτηκε. Ας είμαστε σοβαροί. Όμως είμαι ο πρώτος που το έλυσε και μάλιστα πριν απο 40 χρόνια:\n Η θεμελιώδης διαφορά ανάμεσα στον άνθρωπο και στα ζώα (λιοντάρια, κουνέλια, κόμπρες, άλογα κτλ) είναι η παπάτζα.   Έχεις δει ποτέ ζώο να πουλάει παραμύθι; Θα προσπαθήσει ποτέ η αρκούδα να σε πείσει ότι μπορεί να σε φάει; Η χελώνα θα σου εξηγήσει με επιχειρήματα ότι είναι πιο γρήγορη απο εσένα; Κατάλαβες που το πάω; Αν όχι, δεν πειράζει. Οι μεγάλες ιδέες δεν γίνονται άμεσα κατανοητές.\n   Ο Elon Musk σε μια βραδινή έξοδο στο Lohan Nightclub.   Το μοντέλο μου λοιπόν ξεκινάει με την υπόθεση ότι η μόνη πραγματική δεξιότητα που υπάρχει στον άνθρωπο είναι η παπάτζα. Ομολογώ ότι είναι ισχυρή υπόθεση αλλά όπως είναι γνωστό:\n All models are wrong but some are useful.   Το γράφω στα αγγλικά γιατί το ίδιο στα ελληνικά δεν γράφει ωραία. Δίχως να χάνω χρόνο, προχωρώ στο notation. Ονομάζω ως $PF$ (Priest Factor) τον παράγοντα ο οποίος μετράει την ικανότητα ενός ατόμου στη παπάτζα και θεωρώ ότι παίρνει τιμές στο σύνολο των πραγματικών αριθμών. Υποθέτω ότι η αυτοπεποίθηση ενός ατόμου, την οποία ονομάζω $CF$ (Confidence Factor), εξαρτάται απο την αλληλεπίδρασή του με κάποια άλλα άτομα. Αυτό είναι λογικό αν το σκεφτείς. Πάντα συγκρίνεις τον εαυτό σου με τους άλλους. Είσαι κοντός γιατί οι άλλοι είναι ψηλοί. Είσαι όμορφος γιατί οι άλλοι είναι μπάζα κ.ο.κ. Στη ζωή όλα είναι σχετικά σύμφωνα με τον Einstein (ή τη γυναίκα του αν πιστέψουμε τις διάφορες θωρίες συνομωσίας).\nΤο κρίσιμο σημείο είναι ότι ο $CF$ ενός ατόμου είναι συνάρτηση του $PF$ του ίδιου ατόμου καθώς και των $PF_{i}$ των $i = 1, \\cdots, n$ ατόμων με τα οποία αλληλεπιδρά. Ποιοτικά θα λέγαμε ότι αν βρέθεις σε μια παρέα στην οποία πουλάς τη μεγαλύτερη παπάτζα τότε όλοι σε πιστεύουν και η αυτοπεποίθησή σου πιάνει μέγιστο. Ομοίως αν είσαι ο ηλίθιος της παρέας, ή ισοδύναμα αυτός που έχει τη μικρότερη ικανότητα στο παραμύθι, τότε κανένας δεν σε πιστεύει και επειδή δεν το παίρνεις καν χαμπάρι (αφού οι υπόλοιποι σου πούλησαν το δικό τους ψέμα) πάλι έχεις μεγάλη σιγουριά για τον εαυτό σου. Το πρόβλημα εμφανίζεται όταν βρίσκονται στον ίδιο χώρο άτομα με αντίστοιχα επίπεδα παπάτζας. Τότε στην περίπτωση που είναι και οι δύο καλοί ψεύτες είναι αδύνατο να κοροιδέψει ο ένας τον άλλον. Αντίστοιχα εάν κανένας απο τους δύο δεν διαθέτει αυτή την ικανότητα τότε μένουν αποσβωλομένοι να κοιτούν με το χαρακτηριστικό βλέμα του πιγκουίνου περιμένοντας κάποιος να πεί το πρώτο ψέμα. Το αποτέλεσμα και στα δύο σενάρια είναι η χαμηλή αυτοπεποίθηση. Ποσοτικά η παραπάνω περιγραφή αντιστοιχεί στην εξής σχέση:\n$$CF = \\frac{1}{n} \\displaystyle\\sum_{i=1}^n | PF - PF_{i} |$$\nδηλαδή η αυτοπεποίθηση ενός ατόμου είναι ίση με το άθροισμα των αποστάσεων ανάμεσα στη παπάτζα του και στην παπάτζα των υπολοίπων. Αυτό είναι πραγματικά deep result. Very deep.\nΥποθέτω δίχως δισταγμό ότι ο $PF_{i}$ για οποιοδήποτε απο αυτά τα ανεξάρτητα άτομα ακολουθεί μια τυποποιημένη κανονική κατανομή, δηλαδή:\n$$PF_{i} \\stackrel{i.i.d.}{\\sim} N(0, 1)$$\nΗ ποσότητα που μας ενδιαφέρει είναι η αναμενόμενη τιμή του $CF$. Απο τις δύο προηγούμενες σχέσεις προκύπτει ότι ο $CF$ ακολουθεί τη λεγόμενη ως διπλωμένη κανονική κατανομή και συνεπώς:\n$E[CF] = \\frac{2}{\\pi} exp \\Big( - \\frac{PF}{2} \\Big) - PF \\cdot erf \\Big( - \\frac{PF}{\\sqrt{2}} \\Big)$\nόπου $erf$ η συνάρτηση σφάλματος.\nΘα σχεδιάσω την αναμενόμενη τιμή $E[CF]$ ως συνάρτηση του $PF$ για τιμές γύρω απο το μηδεν και τρείς τυπικές αποκλίσεις. Το παρακάτω απόσπασμα κώδικα σε Python 3 παράγει το ζητούμενο γράφημα:\n# Imports from math import sqrt, exp, erf import numpy as np import matplotlib.pyplot as plt # Define expectation of confidence factors def calculate_confidence_factors(priest_factors): values = (2.0 / np.pi) * np.exp(-priest*factors / 2) corrections = priest_factors * np.erf(-priest_factors / np.sqrt(2)) return values - corrections # Generate priest and confidence factors values priest_factors = np.linspace(-3.0, 3.0, 20) expected_confidence_factors = calculate_confidence_factors(priest_factors) # Plot fig, ax = plt.subplots() ax.plot(priest_factors, expected_confidence_factors) ax.set( xlabel='Priest Factor', ylabel='Expected Confidence Factor', title='Dunning-Kruger Effect' ) ax.grid() plt.show()  Η ομοιότητα με το υποτιθέμενο Ντάνινγκ–Κρούγκερ γράφημα είναι χωρίς υπερβολές ανατριχιαστική. Απο την συγκίνηση δυσκολέυομαι να γράψω ακόμα και τον επίλογο αυτού του άρθρου. Θα χαρακτήριζα την ανακάλυψη του μοντέλου ως μια μεταφυσική εμπειρία η οποία με άλλαξε ως άνθρωπο. Συγκεκριμένα μου προκάλεσε μια αστρική προβολή η οποία με ταξίδεψε στα όρια του γαλαξία μας. Ας αφήσω καλύτερα τις λεπτομέρειες για ένα επόμενο άρθρο.\n","date":1553731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553731200,"objectID":"4748e35de2ce105ea5aa5242392fcafe","permalink":"https://georgedouzas.github.io/site/post/dunning-krugger/","publishdate":"2019-03-28T00:00:00Z","relpermalink":"/site/post/dunning-krugger/","section":"post","summary":"Όταν μια απλή καράφλα δεν ειναι ικανή να σε σταματήσει.","tags":["Other"],"title":"Το φαινόμενο Dunning-Krugger και ο Ηλίας Μάσκας","type":"post"},{"authors":["Georgios Douzas","Fernando Bacao","Felix Last"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"767335a965d69570ede98e300e9ed2b4","permalink":"https://georgedouzas.github.io/site/publication/kmeans_smote_journal/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/site/publication/kmeans_smote_journal/","section":"publication","summary":"Learning from class-imbalanced data continues to be a common and challenging problem in supervised learning as standard classification algorithms are designed to handle balanced class distributions. While different strategies exist to tackle this problem, methods which generate artificial data to achieve a balanced class distribution are more versatile than modifications to the classification algorithm. Such techniques, called oversamplers, modify the training data, allowing any classifier to be used with class-imbalanced datasets. Many algorithms have been proposed for this task, but most are complex and tend to generate unnecessary noise. This work presents a simple and effective oversampling method based on k-means clustering and SMOTE (synthetic minority oversampling technique), which avoids the generation of noise and effectively overcomes imbalances between and within classes. Empirical results of extensive experiments with 90 datasets show that training data oversampled with the proposed method improves classification results. Moreover, k-means SMOTE consistently outperforms other popular oversampling methods. An implementation1 is made available in the Python programming language.","tags":["Machine Learning","Imbalanced Learning Problem","SMOTE","K-Means"],"title":"Improving imbalanced learning through a heuristic oversampling method based on k-means and SMOTE","type":"publication"},{"authors":["Georgios Douzas","Fernando Bacao"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"54d81ab9b74cc82e55fc8c28d9d58430","permalink":"https://georgedouzas.github.io/site/publication/cgan/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/site/publication/cgan/","section":"publication","summary":"Learning from imbalanced datasets is a frequent but challenging task for standard classification algorithms. Although there are different strategies to address this problem, methods that generate artificial data for the minority class constitute a more general approach compared to algorithmic modifications. Standard oversampling methods are variations of the SMOTE algorithm, which generates synthetic samples along the line segment that joins minority class samples. Therefore, these approaches are based on local information, rather on the overall minority class distribution. Contrary to these algorithms, in this paper the conditional version of Generative Adversarial Networks (cGAN) is used to approximate the true data distribution and generate data for the minority class of various imbalanced datasets. The performance of cGAN is compared against multiple standard oversampling algorithms. We present empirical results that show a significant improvement in the quality of the generated data when cGAN is used as an oversampling algorithm.","tags":["Machine Learning","Imbalanced Learning Problem","SMOTE","SOM"],"title":"Effective data generation for imbalanced learning using conditional generative adversarial networks","type":"publication"},{"authors":["Georgios Douzas","Felix Last","Fernando Bacao"],"categories":null,"content":"","date":1513036800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513036800,"objectID":"25891c723a420ef22d6dd0ac156019e9","permalink":"https://georgedouzas.github.io/site/publication/kmeans_smote_preprint/","publishdate":"2017-12-12T00:00:00Z","relpermalink":"/site/publication/kmeans_smote_preprint/","section":"publication","summary":"Learning from class-imbalanced data continues to be a common and challenging problem in supervised learning as standard classification algorithms are designed to handle balanced class distributions. While different strategies exist to tackle this problem, methods which generate artificial data to achieve a balanced class distribution are more versatile than modifications to the classification algorithm. Such techniques, called oversamplers, modify the training data, allowing any classifier to be used with class-imbalanced datasets. Many algorithms have been proposed for this task, but most are complex and tend to generate unnecessary noise. This work presents a simple and effective oversampling method based on k-means clustering and SMOTE oversampling, which avoids the generation of noise and effectively overcomes imbalances between and within classes. Empirical results of extensive experiments with 71 datasets show that training data oversampled with the proposed method improves classification results. Moreover, k-means SMOTE consistently outperforms other popular oversampling methods. An implementation is made available in the python programming language.","tags":["Machine Learning","Imbalanced Learning Problem","SMOTE","K-Means"],"title":"Oversampling for Imbalanced Learning Based on K-Means and SMOTE","type":"publication"},{"authors":["Georgios Douzas","Fernando Bacao"],"categories":null,"content":"","date":1506816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506816000,"objectID":"636136779b952830b2ef81e654de9bec","permalink":"https://georgedouzas.github.io/site/publication/somo/","publishdate":"2017-10-01T00:00:00Z","relpermalink":"/site/publication/somo/","section":"publication","summary":"Learning from imbalanced datasets is challenging for standard algorithms, as they are designed to work with balanced class distributions. Although there are different strategies to tackle this problem, methods that address the problem through the generation of artificial data constitute a more general approach compared to algorithmic modifications. Specifically, they generate artificial data that can be used by any algorithm, not constraining the options of the user. In this paper, we present a new oversampling method, Self-Organizing Map-based Oversampling (SOMO), which through the application of a Self-Organizing Map produces a two dimensional representation of the input space, allowing for an effective generation of artificial data points. SOMO comprises three major stages. Initially a Self-Organizing Map produces a two-dimensional representation of the original, usually high-dimensional, space. Next it generates within-cluster synthetic samples and finally it generates between cluster synthetic samples. Additionally we present empirical results that show the improvement in the performance of algorithms, when artificial data generated by SOMO are used, and also show that our method outperforms various oversampling methods.","tags":["Machine Learning","Imbalanced Learning Problem","SMOTE","SOM"],"title":"Self-Organizing Map Oversampling (SOMO) for imbalanced data set learning","type":"publication"},{"authors":["Georgios Douzas","Fernando Bacao"],"categories":null,"content":"","date":1505952000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505952000,"objectID":"10aa9eefc8568a7894721d26b5c70838","permalink":"https://georgedouzas.github.io/site/publication/gsmote_preprint/","publishdate":"2017-09-21T00:00:00Z","relpermalink":"/site/publication/gsmote_preprint/","section":"publication","summary":"Classification of imbalanced datasets is a challenging task for standard algorithms. Although many methods exist to address this problem in different ways, generating artificial data for the minority class is a more general approach compared to algorithmic modifications. SMOTE algorithm and its variations generate synthetic samples along a line segment that joins minority class instances. In this paper we propose Geometric SMOTE (G-SMOTE) as a generalization of the SMOTE data generation mechanism. G-SMOTE generates synthetic samples in a geometric region of the input space, around each selected minority instance. While in the basic configuration this region is a hyper-sphere, G-SMOTE allows its deformation to a hyper-spheroid and finally to a line segment, emulating, in the last case, the SMOTE mechanism. The performance of G-SMOTE is compared against multiple standard oversampling algorithms. We present empirical results that show a significant improvement in the quality of the generated data when G-SMOTE is used as an oversampling algorithm.","tags":["Machine Learning","Imbalanced Learning Problem","Geometric SMOTE"],"title":"Geometric SMOTE: Effective oversampling for imbalanced learning through a geometric extension of SMOTE","type":"publication"},{"authors":["Georgios Douzas","Theodoros Grammatikopoulos","George Zoupanos"],"categories":null,"content":"","date":1229040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1229040000,"objectID":"b11203463781994f5035f943543c989d","permalink":"https://georgedouzas.github.io/site/publication/csdr2/","publishdate":"2008-12-12T00:00:00Z","relpermalink":"/site/publication/csdr2/","section":"publication","summary":"We consider a E=1 supersymmetric E(8) gauge theory, defined in ten dimensions and we determine all four-dimensional gauge theories resulting from the generalized dimensional reduction à la Forgacs–Manton over coset spaces, followed by a subsequent application of the Wilson flux spontaneous symmetry-breaking mechanism. Our investigation is constrained only by the requirements that (i) the dimensional reduction leads to the potentially phenomenologically interesting, anomaly-free, four-dimensional E(6), SO(10) and SU(5) GUTs and (ii) the Wilson flux mechanism makes use only of the freely acting discrete symmetries of all possible six-dimensional coset spaces.","tags":["Physics"],"title":"Coset space dimensional reduction and Wilson flux breaking of ten-dimensional N=1, E(8) gauge theory","type":"publication"},{"authors":["Georgios Douzas","Theodoros Grammatikopoulos","John Madore","George Zoupanos"],"categories":null,"content":"","date":1207612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1207612800,"objectID":"530c925582adcfd7e1b0fae0a57b1106","permalink":"https://georgedouzas.github.io/site/publication/csdr/","publishdate":"2008-04-08T00:00:00Z","relpermalink":"/site/publication/csdr/","section":"publication","summary":"Starting from a Yang‐Mills‐Dirac theory defined in ten dimensions we classify the semi‐realistic particle physics models resulting from their Forgacs‐Manton dimensional reduction. The higher‐dimensional gauge group is chosen to be $\\mathrm{E(8)}$. This choice as well as the dimensionality of the space‐time is suggested by the heterotic string theory. Furthermore, we assume that the space‐time on which the theory is defined can be written in the compactified form $\\mathrm{M}^{4} \\times \\mathrm{B}$, with $\\mathrm{M}^{4}$ the ordinary Minkowski spacetime and $\\mathrm{B} = \\mathrm{S/R}$ a 6‐dim homogeneous coset space. We constrain our investigation in those cases where the dimensional reduction leads in four dimensions to phenomenologically interesting and anomaly‐free GUTs such as $\\mathrm{E(6)}$, $\\mathrm{SO(10)}$ and $\\mathrm{SU(5)}$. However the four‐dimensional surviving scalars transform in the fundamental of the resulting gauge group are not suitable for the superstrong symmetry breaking of the Standard Model. The main objective of our work is the investigation to which extent the latter can be achieved by employing the Wilson flux breaking mechanism.","tags":["Physics"],"title":"Coset space dimensional reduction and classification of semi‐realistic particle physics models","type":"publication"}]